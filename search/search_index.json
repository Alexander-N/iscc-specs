{
    "docs": [
        {
            "location": "/",
            "text": "ISCC - Content Identifier\n#\n\n\nThe ISCC (International Standard Content Code) is a modern, generic, and free content identifier:\n\n\n\n\nMotivation\n#\n\n\nThe media industry is still mostly relying on identifiers that were originally designed for physical products such as printed books and magazines. However, traditional content identifiers (like ISBN, ISSN or ISRC) are managed centrally and fall short of the requirements for digital trade.\n\n\nFreely accessible \nstandard identifiers\n, which are specifically designed to manage content in our digital century are a fundamental prerequisite for \nblockchain\n based transactions and sales activities in an increasingly heterogeneous media environment.\n\n\nWith better identifiers for digital content, the entire ecosystem becomes more efficient.\n\n\nKey Differentiators\n#\n\n\n\n\n\n\n\n\nExisting Media Identifiers\n\n\nISCC\n Content Identifier\n\n\n\n\n\n\n\n\n\n\nCentralized issuance\n\n\nDecentralized issuance\n\n\n\n\n\n\nIndustry specific overspecialization\n\n\nGeneric content identifier\n\n\n\n\n\n\nNone or human curated semantics\n\n\nAlgorithmic similarity & deduplication\n\n\n\n\n\n\nHigh management costs\n\n\nLow management costs\n\n\n\n\n\n\nHigh barrier of entry\n\n\nLow barrier of entry\n\n\n\n\n\n\nNot designed for blockchain storage\n\n\nDesigned for and registered on blockchain\n\n\n\n\n\n\n\n\nHow it works\n#\n\n\nISCC\n identifiers are generated algorithmically from a basic set of metadata and the content itself. The ISCC does not have to be carried explicitly with the content because the content itself is the authority of the \nISCC Code\n.\n\n\nThe \nISCC Code\n is a unique, hierarchically structured composite identifier. It is built from a generic and balanced mix of content-derived, locality-sensitive and similarity-preserving hashes generated from metadata and content.\n\n\n\n\nThe latest version of these pages can be found at \niscc.codes",
            "title": "Overview"
        },
        {
            "location": "/#iscc-content-identifier",
            "text": "The ISCC (International Standard Content Code) is a modern, generic, and free content identifier:",
            "title": "ISCC - Content Identifier"
        },
        {
            "location": "/#motivation",
            "text": "The media industry is still mostly relying on identifiers that were originally designed for physical products such as printed books and magazines. However, traditional content identifiers (like ISBN, ISSN or ISRC) are managed centrally and fall short of the requirements for digital trade.  Freely accessible  standard identifiers , which are specifically designed to manage content in our digital century are a fundamental prerequisite for  blockchain  based transactions and sales activities in an increasingly heterogeneous media environment.  With better identifiers for digital content, the entire ecosystem becomes more efficient.",
            "title": "Motivation"
        },
        {
            "location": "/#key-differentiators",
            "text": "Existing Media Identifiers  ISCC  Content Identifier      Centralized issuance  Decentralized issuance    Industry specific overspecialization  Generic content identifier    None or human curated semantics  Algorithmic similarity & deduplication    High management costs  Low management costs    High barrier of entry  Low barrier of entry    Not designed for blockchain storage  Designed for and registered on blockchain",
            "title": "Key Differentiators"
        },
        {
            "location": "/#how-it-works",
            "text": "ISCC  identifiers are generated algorithmically from a basic set of metadata and the content itself. The ISCC does not have to be carried explicitly with the content because the content itself is the authority of the  ISCC Code .  The  ISCC Code  is a unique, hierarchically structured composite identifier. It is built from a generic and balanced mix of content-derived, locality-sensitive and similarity-preserving hashes generated from metadata and content.   The latest version of these pages can be found at  iscc.codes",
            "title": "How it works"
        },
        {
            "location": "/concept/",
            "text": "ISCC - Concept\n#\n\n\nThe internet is shifting towards a network of decentralized peer-to-peer transactions. If we want our transactions on the emerging blockchain networks to be about content we need standardized ways to address content. Our transactions might be payments, attributions, reputation, certification, licenses or entirely new kinds of value transfer. All this will happen much faster and easier if we, as a community, can agree on how to identify content in a decentralized environment. This is the first draft of an open proposal to the wider content community for a common content identifier. We would like to share our ideas and spark a conversation with journalists, news agencies, content creators, publishers, distributors, libraries, musicians, scientists, developers, lawyers, rights organizations and all the other participants of the content ecosystem.\n\n\nIntroduction\n#\n\n\nThere are many\n existing standards\n for media identifiers serving a wide array of use cases. For example book publishing uses the \nISBN\n, magazines have the \nISSN\n, music industry has \nISRC\n, film has \nISAN\n and science has \nDOI\n - each of them serving a set of specific purposes. These identifiers have important roles across many layers. The \nstructure and management\n of these \nglobal identifiers\n strongly correlates with the grade of achievable \nautomation\n and potential for \ninnovation\n within and across different sectors of the media industries. Some communities, like online journalism, don't even have any global persistent identifiers for their content.\n\n\nMany of the established standards manage registration of identifiers in \ncentralized \nor \nhierarchical systems\n involving manual and costly processes. Often the associated metadata is not easily or freely accessible for third parties (if available at all). The overhead, cost and general properties of these systems make them unsuitable for many innovative use cases. Existing and established standards have trouble keeping up with the fast evolving digital economy. For example nowadays major e-book retailers do not even require an \nISBN\n and instead establish their own proprietary identifiers. Amazon has the \nASIN\n, Apple has \nApple-ID\n and Google has \nGKEY\n. The fast paced development of the digital media economy has led to an increasing fragmentation of identifiers and new barriers in interoperability. For many tasks current systems need to track and match all the different vendor specific IDs, which is an inefficient and error prone process.\n\n\nAdvances in data structures, algorithms, machine learning and the uprise of crypto economics allows us to invent \nnew\n kinds of \nmedia identifiers\n and \nre-imagine existing identifiers\n with innovative use cases in mind. Blockchains and Smart Contracts offer great opportunities in solving many of the challenges of identifier registration, like centralized management, data duplication and disambiguation, vendor lock-in and long term data retention.\n\n\nThis is an open proposal to the digital media community and explores the possibilities of a \ndecentralized \ncontent identifier system. We\u2019d like to establish an open standard for persistent, unique, vendor independent and content derived cross-media identifiers that are stored and managed in a global and decentralized blockchain. We envision a self-governing ecosystem with a low barrier of entry where \ncommercial and non-commercial\n initiatives can both innovate and thrive next to each other.\n\n\nMedia Identifiers for Blockchains\n#\n\n\nMedia cataloging systems tend to get out of hand and become complex and often unmanageable. Our design proposal is focused on keeping the ISCC system as simple and more importantly as \nautomatable\n as possible, while maximizing practical value for the most important use cases \u2014 meaning you should get out more than you have to put in. With this in mind we come to the following basic design decisions:\n\n\nA \u201cMeaningful\u201d Identifier\n#\n\n\nIn traditional database systems it is recommended practice to work with \nsurrogate keys\n as identifiers. A surrogate key has no business meaning and is completely decoupled from the data it identifies. Uniqueness of such identifiers is guaranteed either via centralized incremental assignment by the database system or via random UUIDs which have a very low probability of collisions. While random UUIDs could be generated in a decentralized way, both approaches require some external authority that establishes or certifies the linkage between the identifier and the associated metadata and content. This is why we decided to go with a \u201cmeaningful\u201d \ncontent and metadata derived identifier (CMDI)\n. Anyone will be able to verify that a specific identifier indeed belongs to a given digital content. Even better, anyone can \u201cfind\u201d the identifier for a given content without the need to consult external data sources. This approach also captures essential information about the media in the identifier itself, which is very useful in scenarios of machine learning and data analytics.\n\n\nA Decentralized Identifier\n#\n\n\nWe would like our identifier to be registry agnostic. This means that identifiers can be self-issued in a decentralized and parallel fashion without the need to ask for permission. Even if identifiers are not registered in a central database or on a public blockchain they are still useful in cases where multiple independent parties exchange information about content. The CMDI approach is helpful with common issues like data integrity, validation, de-duplication and disambiguation.\n\n\nStorage Considerations\n#\n\n\nOn a typical public blockchain all data is \nfully replicated\n among participants. This allows for independent and autonomous validation of transactions. All blockchain data is highly available, tamper-proof and accessible for free. However, under high load the limited transaction capacity (storage space per unit of time) creates a transaction fee market. This leads to\n growing transaction costs\n and makes storage space a scarce and increasingly precious resource. So it is mandatory for our identifier and its eventual metadata schema to be very \nspace efficient \nto maximize benefit at minimum cost. The basic metadata that will be required to generate and register identifiers must be:\n\n\n\n\nminimal in scope\n\n\nclearly specified\n\n\nrobust against human error\n\n\nenforced on technical level\n\n\nadequate for public use (no legal or privacy issues)\n\n\n\n\nLayers of Digital Media Identification\n#\n\n\nWhile we examined existing identifiers we discovered that there is often much confusion about the extent or coverage of what exactly is being identified by a given system. With our idea for a generic cross-media identifier we want to put special weight on being precise with our definitions and found it helpful to distinguish between \u201cdifferent layers of digital media identification\". We found that these layers exist naturally on a scale from abstract to concrete. Our analysis also showed that existing standard identifiers only operate on one or at most two of such layers. The ISCC will be designed as a \ncomposite identifier\n that takes the different layers of media identification into consideration:\n\n\nLayer 1 \u2013 Abstract Creation\n#\n\n\nIn the first and most abstract layer we are concerned with distinguishing between different works or creations in the \nbroadest possible sense\n. The scope of identification is completely independent of any manifestations of the work, be it physical or digital in nature. It is also agnostic to creators, rights holders or any specific interpretations, expressions or language versions of a work. It only relates to the intangible creation - the idea itself.\n\n\nLayer 2 \u2013 Semantic Field\n#\n\n\nThis layer relates to the meaning or essence of a work. It is an amorphous collection or combination of facts, concepts, categories, subjects, topics, themes, assumptions, observations, conclusions, beliefs and other intangible things that the content conveys. The scope of identification is a set of coordinates within a finite and multidimensional semantic space.\n\n\nLayer 3 \u2013 Generic Manifestation\n#\n\n\nIn this layer we are concerned with the literal structure of a media type specific and normalized manifestation. Namely the basic text, image, audio or video content independent of its semantic meaning or media file encoding and with a tolerance to variation. This \"tolerance to variation\" bundles a set of different versions with corrections, revisions, edits, updates, personalizations, different format encodings or data compressions of the same content under one grouping identifier. A generic manifestation is independent of a final digital media product and is specific to an expression, version or interpretation of a work.\n\n\nUnfortunately it is not obvious where generic manifestation of a work ends and another one starts. It depends on human interpretation and context. How much editing do we allow before we call it a \u201cdifferent\u201d manifestation and give it a different identifier. A practical but only partial solution to this problem is to create a algorithmically defined and testable spectrum of tolerance to variation per media type. This can provide a stable and repeatable process to distinguish between generic content manifestations. But it is important to understand that such a process is not expected to yield results that are always intuitive to human expectations as to where exactly boundaries should be.\n\n\nLayer 4 \u2013 Media Specific Manifestation\n#\n\n\nThis layer relates to a \nmanifestation with a specific encoding\n. It identifies a \ndata-file\n encoded and offered in a specific \nmedia format \nincluding a tolerance to variation to account for minor edits and updates within a format without creating a new identifier. For example one could distinguish between the PDF, DOCX or WEBSITE versions of the same content as generated from a single source publishing system. This layer does only distinguish between products or \"artifacts\" with a given packaging or encoding.\n\n\nLayer 5 \u2013 Exact Representation\n#\n\n\nIn this layer we identify a data-file by its exact binary representation without any interpretation of meaning and without any ambiguity. Even a minimal change in data that might not change the interpretation of content would create a different identifier. Like the first four layers, this layer does also \nnot \nexpress any information related to \ncontent location\n or \nownership\n.\n\n\nLayer 6 \u2013 Individual Copy\n#\n\n\nIn the physical world we would call a specific book (one that you can take out of your shelve) an \nindividual copy\n. This implies a notion of \nlocality \nand \nownership\n. In the digital world the semantics of an individual copy are very different. An individual copy might be distinguished by a license you own or by a personalized watermark applied by the retailer at time of sale or some digital annotations you have added to your digital media file. While there can only ever be \none exact\n individual copy of a \nphysical object\n, there always can be \nendless replicas\n of an \"individual copy\" of a \ndigital object\n. It is very important to keep this difference in mind. Ignoring this fact has caused countless misunderstandings and is the source of confusion throughout the media industry \u2013 especially in realm of copyright and license discussions.\n\n\nWe could try to define an \nindividual digital copy\n by its location and exact content on a specific physical storage medium (like a DVD, SSD ...). But this does not account for the fact that it is nearly impossible to stop someone from creating an exact replica of that data or at least a snapshot or recording of the presentation of that data on another storage location.\n\n\nAnd most importantly such a replica does not affect the original data and even less can make it magically disappear. In contrast, if you give your individual copy of your book to someone else, you won't \n\"have it\"\n anymore. It is clear, that with digital media this \ncannot reliably be the case\n. The only way would be to build a \ntamper-proof physical device\n (secure element) that does not reveal the data itself, which would defeat the purpose by making the content itself unavailable. But there are ways to partially simulate such inherently physical properties in the digital world. Most notably with the emergence of blockchain technology it is now possible to have a \ncryptographically secured\n and publicly notarized tamper-proof \ncertificate of ownership. \n This can serve as a record of agreement about ownership of an \u201cindividual copy\u201d. But is does not by itself enforce location or accessibility of the content, nor does it prove the authorization of the certifying party itself or the legal validity of the agreement.\n\n\nAlgorithmic Tools\n#\n\n\nWhile many details about the ISCC are still up for discussion we are quite confident about some of the general algorithmic families that will make it into the final specification for the identifier. These will play an important role in how we generate the different components of the identifier:\n\n\n\n\nSimilarity preserving hash functions (Simhash, Minhash ...)\n\n\nPerceptual hashing (pHash, Blockhash, Chromaprint \u2026)\n\n\nContent defined chunking (Rabin-Karp, FastCDC ...)\n\n\nMerkle trees\n\n\n\n\nISCC Proof-of-Concept\n#\n\n\nBefore we settle on the details of the proposed ISCC identifier, we want to build a simple and reduced proof-of-concept implementation of our ideas. It will enable us and other developers to test with real world data and systems and find out early what works and what doesn't.\n\n\n\n\n\n\nUpdate\n\n\nAn interactive demo of the concept is available at \nhttps://isccdemo.content-blockchain.org/\n\n\n\n\nThe minimal viable, first iteration ISCC will be a byte structure built from the following components:\n\n\nMeta-ID\n#\n\n\nThe MetaID will be generated as a similarity preserving hash from minimal generic metadata like \ntitle \nand \ncreators\n. It operates on \nLayer 1 \n and identifies an intangible creation. It is the first and most generic grouping element of the identifier. We will be experimenting with different n-gram sizes and bit-length to find the practical limits of precision and recall for generic metadata. We will also specify a process to disambiguate unintended collisions by adding optional metadata.\n\n\nPartial Content Flag\n#\n\n\nThe Partial Content Flag is a 1-bit flag that indicates if the remaining elements relate to the complete work or only to a subset of it.\n\n\nMedia Type Flag\n#\n\n\nThe Media Type Flag is a 3 bit flag that allows us to distinguish between up to 8 generic media types\n (GMTs)\n to which our ContentID component applies. We define a generic media type as\nbasic content type\n such as plain text or raw pixel data that will be specified exactly and extracted from more complex file formats or encodings. We will start with generic text and image types and add audio, video and mixed types later.\n\n\nContent-ID\n#\n\n\nThe ContentID operates on \nLayer 3\n and will be a GMT-specific similarity preserving hash generated from extracted content. It identifies the normalized content of a specific GMT, independent of file format or encoding. It relates to the structural essence of the content and groups similar GMT-specific manifestations of the abstract creation or parts of it (as indicated by the Partial Content Flag). For practical reasons we intentionally skip a \nLayer 2\n component at this time. It would add unnecessary complexity for a basic proof-of-concept implementation.\n\n\nData-ID\n#\n\n\nThe DataID operates on \nLayer 4 \nand will be a similarity preserving hash generated from shift-resistant content-defined chunks from the raw data of the encoded media blob. It groups complete encoded files with similar content and encoding. This component does not distinguish between GMTs as the files may include multiple different generic media types.\n\n\nInstance-ID\n#\n\n\nThe InstanceID operates on \nLayer 5 \nand will be the top hash of a merkle tree generated from (potentially content-defined) chunks of raw data of an encoded media blob. It identifies a concrete manifestation and proves the integrity of the full content. We use the merkle tree structure because it also allows as to verify integrity of partial chunks without having to have the full data available. This will be very useful in any scenarios of distributed data storage.\n\n\nWe intentionally skip \nLayer 6\n at this stage as content ownership and location will be handled on the blockchain layer of the stack and not by the ISCC identifier itself.",
            "title": "Concept"
        },
        {
            "location": "/concept/#iscc-concept",
            "text": "The internet is shifting towards a network of decentralized peer-to-peer transactions. If we want our transactions on the emerging blockchain networks to be about content we need standardized ways to address content. Our transactions might be payments, attributions, reputation, certification, licenses or entirely new kinds of value transfer. All this will happen much faster and easier if we, as a community, can agree on how to identify content in a decentralized environment. This is the first draft of an open proposal to the wider content community for a common content identifier. We would like to share our ideas and spark a conversation with journalists, news agencies, content creators, publishers, distributors, libraries, musicians, scientists, developers, lawyers, rights organizations and all the other participants of the content ecosystem.",
            "title": "ISCC - Concept"
        },
        {
            "location": "/concept/#introduction",
            "text": "There are many  existing standards  for media identifiers serving a wide array of use cases. For example book publishing uses the  ISBN , magazines have the  ISSN , music industry has  ISRC , film has  ISAN  and science has  DOI  - each of them serving a set of specific purposes. These identifiers have important roles across many layers. The  structure and management  of these  global identifiers  strongly correlates with the grade of achievable  automation  and potential for  innovation  within and across different sectors of the media industries. Some communities, like online journalism, don't even have any global persistent identifiers for their content.  Many of the established standards manage registration of identifiers in  centralized  or  hierarchical systems  involving manual and costly processes. Often the associated metadata is not easily or freely accessible for third parties (if available at all). The overhead, cost and general properties of these systems make them unsuitable for many innovative use cases. Existing and established standards have trouble keeping up with the fast evolving digital economy. For example nowadays major e-book retailers do not even require an  ISBN  and instead establish their own proprietary identifiers. Amazon has the  ASIN , Apple has  Apple-ID  and Google has  GKEY . The fast paced development of the digital media economy has led to an increasing fragmentation of identifiers and new barriers in interoperability. For many tasks current systems need to track and match all the different vendor specific IDs, which is an inefficient and error prone process.  Advances in data structures, algorithms, machine learning and the uprise of crypto economics allows us to invent  new  kinds of  media identifiers  and  re-imagine existing identifiers  with innovative use cases in mind. Blockchains and Smart Contracts offer great opportunities in solving many of the challenges of identifier registration, like centralized management, data duplication and disambiguation, vendor lock-in and long term data retention.  This is an open proposal to the digital media community and explores the possibilities of a  decentralized  content identifier system. We\u2019d like to establish an open standard for persistent, unique, vendor independent and content derived cross-media identifiers that are stored and managed in a global and decentralized blockchain. We envision a self-governing ecosystem with a low barrier of entry where  commercial and non-commercial  initiatives can both innovate and thrive next to each other.",
            "title": "Introduction"
        },
        {
            "location": "/concept/#media-identifiers-for-blockchains",
            "text": "Media cataloging systems tend to get out of hand and become complex and often unmanageable. Our design proposal is focused on keeping the ISCC system as simple and more importantly as  automatable  as possible, while maximizing practical value for the most important use cases \u2014 meaning you should get out more than you have to put in. With this in mind we come to the following basic design decisions:",
            "title": "Media Identifiers for Blockchains"
        },
        {
            "location": "/concept/#a-meaningful-identifier",
            "text": "In traditional database systems it is recommended practice to work with  surrogate keys  as identifiers. A surrogate key has no business meaning and is completely decoupled from the data it identifies. Uniqueness of such identifiers is guaranteed either via centralized incremental assignment by the database system or via random UUIDs which have a very low probability of collisions. While random UUIDs could be generated in a decentralized way, both approaches require some external authority that establishes or certifies the linkage between the identifier and the associated metadata and content. This is why we decided to go with a \u201cmeaningful\u201d  content and metadata derived identifier (CMDI) . Anyone will be able to verify that a specific identifier indeed belongs to a given digital content. Even better, anyone can \u201cfind\u201d the identifier for a given content without the need to consult external data sources. This approach also captures essential information about the media in the identifier itself, which is very useful in scenarios of machine learning and data analytics.",
            "title": "A \u201cMeaningful\u201d Identifier"
        },
        {
            "location": "/concept/#a-decentralized-identifier",
            "text": "We would like our identifier to be registry agnostic. This means that identifiers can be self-issued in a decentralized and parallel fashion without the need to ask for permission. Even if identifiers are not registered in a central database or on a public blockchain they are still useful in cases where multiple independent parties exchange information about content. The CMDI approach is helpful with common issues like data integrity, validation, de-duplication and disambiguation.",
            "title": "A Decentralized Identifier"
        },
        {
            "location": "/concept/#storage-considerations",
            "text": "On a typical public blockchain all data is  fully replicated  among participants. This allows for independent and autonomous validation of transactions. All blockchain data is highly available, tamper-proof and accessible for free. However, under high load the limited transaction capacity (storage space per unit of time) creates a transaction fee market. This leads to  growing transaction costs  and makes storage space a scarce and increasingly precious resource. So it is mandatory for our identifier and its eventual metadata schema to be very  space efficient  to maximize benefit at minimum cost. The basic metadata that will be required to generate and register identifiers must be:   minimal in scope  clearly specified  robust against human error  enforced on technical level  adequate for public use (no legal or privacy issues)",
            "title": "Storage Considerations"
        },
        {
            "location": "/concept/#layers-of-digital-media-identification",
            "text": "While we examined existing identifiers we discovered that there is often much confusion about the extent or coverage of what exactly is being identified by a given system. With our idea for a generic cross-media identifier we want to put special weight on being precise with our definitions and found it helpful to distinguish between \u201cdifferent layers of digital media identification\". We found that these layers exist naturally on a scale from abstract to concrete. Our analysis also showed that existing standard identifiers only operate on one or at most two of such layers. The ISCC will be designed as a  composite identifier  that takes the different layers of media identification into consideration:",
            "title": "Layers of Digital Media Identification"
        },
        {
            "location": "/concept/#layer-1-abstract-creation",
            "text": "In the first and most abstract layer we are concerned with distinguishing between different works or creations in the  broadest possible sense . The scope of identification is completely independent of any manifestations of the work, be it physical or digital in nature. It is also agnostic to creators, rights holders or any specific interpretations, expressions or language versions of a work. It only relates to the intangible creation - the idea itself.",
            "title": "Layer 1 \u2013 Abstract Creation"
        },
        {
            "location": "/concept/#layer-2-semantic-field",
            "text": "This layer relates to the meaning or essence of a work. It is an amorphous collection or combination of facts, concepts, categories, subjects, topics, themes, assumptions, observations, conclusions, beliefs and other intangible things that the content conveys. The scope of identification is a set of coordinates within a finite and multidimensional semantic space.",
            "title": "Layer 2 \u2013 Semantic Field"
        },
        {
            "location": "/concept/#layer-3-generic-manifestation",
            "text": "In this layer we are concerned with the literal structure of a media type specific and normalized manifestation. Namely the basic text, image, audio or video content independent of its semantic meaning or media file encoding and with a tolerance to variation. This \"tolerance to variation\" bundles a set of different versions with corrections, revisions, edits, updates, personalizations, different format encodings or data compressions of the same content under one grouping identifier. A generic manifestation is independent of a final digital media product and is specific to an expression, version or interpretation of a work.  Unfortunately it is not obvious where generic manifestation of a work ends and another one starts. It depends on human interpretation and context. How much editing do we allow before we call it a \u201cdifferent\u201d manifestation and give it a different identifier. A practical but only partial solution to this problem is to create a algorithmically defined and testable spectrum of tolerance to variation per media type. This can provide a stable and repeatable process to distinguish between generic content manifestations. But it is important to understand that such a process is not expected to yield results that are always intuitive to human expectations as to where exactly boundaries should be.",
            "title": "Layer 3 \u2013 Generic Manifestation"
        },
        {
            "location": "/concept/#layer-4-media-specific-manifestation",
            "text": "This layer relates to a  manifestation with a specific encoding . It identifies a  data-file  encoded and offered in a specific  media format  including a tolerance to variation to account for minor edits and updates within a format without creating a new identifier. For example one could distinguish between the PDF, DOCX or WEBSITE versions of the same content as generated from a single source publishing system. This layer does only distinguish between products or \"artifacts\" with a given packaging or encoding.",
            "title": "Layer 4 \u2013 Media Specific Manifestation"
        },
        {
            "location": "/concept/#layer-5-exact-representation",
            "text": "In this layer we identify a data-file by its exact binary representation without any interpretation of meaning and without any ambiguity. Even a minimal change in data that might not change the interpretation of content would create a different identifier. Like the first four layers, this layer does also  not  express any information related to  content location  or  ownership .",
            "title": "Layer 5 \u2013 Exact Representation"
        },
        {
            "location": "/concept/#layer-6-individual-copy",
            "text": "In the physical world we would call a specific book (one that you can take out of your shelve) an  individual copy . This implies a notion of  locality  and  ownership . In the digital world the semantics of an individual copy are very different. An individual copy might be distinguished by a license you own or by a personalized watermark applied by the retailer at time of sale or some digital annotations you have added to your digital media file. While there can only ever be  one exact  individual copy of a  physical object , there always can be  endless replicas  of an \"individual copy\" of a  digital object . It is very important to keep this difference in mind. Ignoring this fact has caused countless misunderstandings and is the source of confusion throughout the media industry \u2013 especially in realm of copyright and license discussions.  We could try to define an  individual digital copy  by its location and exact content on a specific physical storage medium (like a DVD, SSD ...). But this does not account for the fact that it is nearly impossible to stop someone from creating an exact replica of that data or at least a snapshot or recording of the presentation of that data on another storage location.  And most importantly such a replica does not affect the original data and even less can make it magically disappear. In contrast, if you give your individual copy of your book to someone else, you won't  \"have it\"  anymore. It is clear, that with digital media this  cannot reliably be the case . The only way would be to build a  tamper-proof physical device  (secure element) that does not reveal the data itself, which would defeat the purpose by making the content itself unavailable. But there are ways to partially simulate such inherently physical properties in the digital world. Most notably with the emergence of blockchain technology it is now possible to have a  cryptographically secured  and publicly notarized tamper-proof  certificate of ownership.   This can serve as a record of agreement about ownership of an \u201cindividual copy\u201d. But is does not by itself enforce location or accessibility of the content, nor does it prove the authorization of the certifying party itself or the legal validity of the agreement.",
            "title": "Layer 6 \u2013 Individual Copy"
        },
        {
            "location": "/concept/#algorithmic-tools",
            "text": "While many details about the ISCC are still up for discussion we are quite confident about some of the general algorithmic families that will make it into the final specification for the identifier. These will play an important role in how we generate the different components of the identifier:   Similarity preserving hash functions (Simhash, Minhash ...)  Perceptual hashing (pHash, Blockhash, Chromaprint \u2026)  Content defined chunking (Rabin-Karp, FastCDC ...)  Merkle trees",
            "title": "Algorithmic Tools"
        },
        {
            "location": "/concept/#iscc-proof-of-concept",
            "text": "Before we settle on the details of the proposed ISCC identifier, we want to build a simple and reduced proof-of-concept implementation of our ideas. It will enable us and other developers to test with real world data and systems and find out early what works and what doesn't.    Update  An interactive demo of the concept is available at  https://isccdemo.content-blockchain.org/   The minimal viable, first iteration ISCC will be a byte structure built from the following components:",
            "title": "ISCC Proof-of-Concept"
        },
        {
            "location": "/concept/#meta-id",
            "text": "The MetaID will be generated as a similarity preserving hash from minimal generic metadata like  title  and  creators . It operates on  Layer 1   and identifies an intangible creation. It is the first and most generic grouping element of the identifier. We will be experimenting with different n-gram sizes and bit-length to find the practical limits of precision and recall for generic metadata. We will also specify a process to disambiguate unintended collisions by adding optional metadata.",
            "title": "Meta-ID"
        },
        {
            "location": "/concept/#partial-content-flag",
            "text": "The Partial Content Flag is a 1-bit flag that indicates if the remaining elements relate to the complete work or only to a subset of it.",
            "title": "Partial Content Flag"
        },
        {
            "location": "/concept/#media-type-flag",
            "text": "The Media Type Flag is a 3 bit flag that allows us to distinguish between up to 8 generic media types  (GMTs)  to which our ContentID component applies. We define a generic media type as basic content type  such as plain text or raw pixel data that will be specified exactly and extracted from more complex file formats or encodings. We will start with generic text and image types and add audio, video and mixed types later.",
            "title": "Media Type Flag"
        },
        {
            "location": "/concept/#content-id",
            "text": "The ContentID operates on  Layer 3  and will be a GMT-specific similarity preserving hash generated from extracted content. It identifies the normalized content of a specific GMT, independent of file format or encoding. It relates to the structural essence of the content and groups similar GMT-specific manifestations of the abstract creation or parts of it (as indicated by the Partial Content Flag). For practical reasons we intentionally skip a  Layer 2  component at this time. It would add unnecessary complexity for a basic proof-of-concept implementation.",
            "title": "Content-ID"
        },
        {
            "location": "/concept/#data-id",
            "text": "The DataID operates on  Layer 4  and will be a similarity preserving hash generated from shift-resistant content-defined chunks from the raw data of the encoded media blob. It groups complete encoded files with similar content and encoding. This component does not distinguish between GMTs as the files may include multiple different generic media types.",
            "title": "Data-ID"
        },
        {
            "location": "/concept/#instance-id",
            "text": "The InstanceID operates on  Layer 5  and will be the top hash of a merkle tree generated from (potentially content-defined) chunks of raw data of an encoded media blob. It identifies a concrete manifestation and proves the integrity of the full content. We use the merkle tree structure because it also allows as to verify integrity of partial chunks without having to have the full data available. This will be very useful in any scenarios of distributed data storage.  We intentionally skip  Layer 6  at this stage as content ownership and location will be handled on the blockchain layer of the stack and not by the ISCC identifier itself.",
            "title": "Instance-ID"
        },
        {
            "location": "/specification/",
            "text": "ISCC\n - Specification v0.9.8\n#\n\n\n\n\nAttention\n\n\nThis document is a work in progress draft! It may be updated, replaced, or obsoleted by other documents at any time. This document MUST not be used as reference material or cited other than as \"work in progress\".\n\n\n\n\nAbstract\n#\n\n\nThe \nInternational Standard Content Code (\nISCC\n)\n, is an open and decentralized digital media identifier. An \nISCC\n can be created from digital content and its basic metadata by anybody who follows the procedures of the \nISCC\n specification or by using open source software that supports \nISCC\n creation \nconforming to the \nISCC\n specification\n.\n\n\nNote to Readers\n#\n\n\nFor public discussion of issues for this draft please use the Github issue tracker: \nhttps://github.com/coblo/iscc-specs/issues\n.\n\n\nThe latest published version of this draft can be found at \nhttp://iscc.codes/specification/\n. \n\n\nPublic review, discussion and contributions are welcome.\n\n\nAbout this Document\n#\n\n\nThis document proposes an open and vendor neutral \nISCC\n standard and describes the technical procedures to create and manage \nISCC\n identifiers. The first version of this document is produced as a prototype by the \nContent Blockchain Project\n and received funding from the \nGoogle Digital News Initiative (DNI)\n. The content of this document is determined by its authors in an open and public consensus process.\n\n\nConventions and Terminology\n#\n\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in \n[RFC 2119]\n.\n\n\nDefinitions\n#\n\n\n\n\nBasic Metadata:\n\n\nMinimal set of metadata about the content that is identified by an \nISCC\n. This metadata may impact the derived Meta-ID Component\n\n\nCharacter:\n\n\nThroughout this specification a \ncharacter\n is meant to be interpreted as one Unicode code point. This also means that due to the structure of Unicode a \ncharacter\n is not necessarily a full glyph but might be a combining accent or similar.\n\n\nDigital Media Object:\n\n\nA blob of raw bytes with some media type specific encoding.\n\n\nExtended Metadata:\n\n\nMetadata that is not encoded within the \nISCC\n Meta-ID but may be supplied together with the \nISCC\n.\n\n\nGeneric Media Type:\n\n\nA basic content type such as plain text in a normalized and \ngeneric\n (\nUTF-8\n) encoding format.\n\n\nISCC\n:\n\n\nInternational Standard Content Code\n\n\nISCC Code\n:\n\n\nThe printable text encoded representation of an \nISCC\n\n\nISCC Digest\n:\n\n\nThe raw binary data of an \nISCC\n\n\n\n\nIntroduction\n#\n\n\nAn \nISCC\n permanently identifies content at multiple levels of \ngranularity\n. It is algorithmically generated from basic metadata and the contents of a digital media object. It is designed for being registered and stored on a public and decentralized blockchain. An \nISCC\n for a media object can be created and registered by the content author, a publisher, a service provider or anybody else. By itself the \nISCC\n and its basic registration on a blockchain does not make any statement or claim about authorship or ownership of the identified content.\n\n\nISCC\n Structure\n#\n\n\nA \nFully Qualified \nISCC Digest\n is a fixed size sequence of \n36 bytes (288 bits)\n assembled from multiple sub-components. The \nFully Qualified  \nISCC Code\n is a \n52 \ncharacter\n encoded printable string representation of a complete \nISCC Digest\n. This is a high-level overview of the \nISCC\n creation process:\n\n\n\n\nISCC\n Components\n#\n\n\nThe \nISCC Digest\n is built from multiple self-describing 72-bit components:\n\n\n\n\n\n\n\n\nComponents:\n\n\nMeta-ID\n\n\nContent-ID\n\n\nData-ID\n\n\nInstance-ID\n\n\n\n\n\n\n\n\n\n\nContext:\n\n\nIntangible creation\n\n\nContent similarity\n\n\nData similarity\n\n\nData checksum\n\n\n\n\n\n\nInput:\n\n\nMetadata\n\n\nExtracted  content\n\n\nRaw data\n\n\nRaw data\n\n\n\n\n\n\nAlgorithms:\n\n\nSimilarity Hash\n\n\nType specific\n\n\nCDC\n, Minimum Hash\n\n\nCDC\n, Hash Tree\n\n\n\n\n\n\nSize:\n\n\n72 bits\n\n\n72 bits\n\n\n72 bits\n\n\n72 bits\n\n\n\n\n\n\n\n\nISCC\n components MAY be used separately or in combination by applications for various purposes. Individual components MUST be presented as 13-\ncharacter\n \nbase58-iscc\n encoded strings to end users and MAY be prefixed with their component name.\n\n\n\n\nSingle component \nISCC\n-Code (13 characters)\n\n\nMeta-ID\n: 11RDXN6ea57QT\n\n\n\n\nCombinations of components MUST include the Meta-ID component and MUST be ordered as \nMeta-ID\n, \nContent-ID\n, \nData-ID\n, and \nInstance-ID\n. Individual components MAY be skipped and SHOULD be separated with hyphens. A combination of components SHOULD be prefixed with \"\nISCC\n\".\n\n\n\n\nCombination of \nISCC\n-Code components\n\n\nISCC\n: 11RDXN6ea57QT-1HWMGKrLki1fL-1ZYcY8d3uTYj8\n\n\n\n\nA \nFully Qualified \nISCC Code\n is an ordered sequence of Meta-ID, Content-ID, Data-ID, and Instance-ID codes. It SHOULD be prefixed with \nISCC\n and MAY be seperated by hypens.\n\n\n\n\nFully Qualified \nISCC\n-Code (52 characters)\n\n\nISCC\n: 11RDXN6ea57QT1HWMGKrLki1fL1ZYcY8d3uTYj81qYuuCMStgpjs\n\n\n\n\n\n\nFully Qualified \nISCC\n-Code with hyphens (55 characters)\n\n\nISCC\n: 11RDXN6ea57QT-1HWMGKrLki1fL-1ZYcY8d3uTYj8-1qYuuCMStgpjs\n\n\n\n\nComponent Types\n#\n\n\nEach component has the same basic structure of a \n1-byte header\n and a \n8-byte body\n section. \n\n\nThe 1-byte header of each component is subdivided into 2 nibbles (4 bits). The first nibble specifies the component type while the second nibble is component specific.\n\n\nThe header only needs to be carried in the encoded representation. As similarity searches accross different components are of little use, the type information contained in the header of each component can be safely ignored after an \nISCC\n has been decomposed and internaly typed by an application. \n\n\nList of Component Headers\n#\n\n\n\n\n\n\n\n\nComponent\n\n\nNibble-1\n\n\nNibble-2\n\n\nByte\n\n\n\n\n\n\n\n\n\n\nMeta-ID\n\n\n0000\n\n\n0000 - \nISCC\n version 1\n\n\n0x00\n\n\n\n\n\n\nContent-ID-Text\n\n\n0001\n\n\n0000 - Content Type Text\n\n\n0x10\n\n\n\n\n\n\nContent-ID-Text \nPCF\n\n\n0001\n\n\n0001 - Content Type Text  + \nPCF\n\n\n0x11\n\n\n\n\n\n\nContent-ID-Image\n\n\n0001\n\n\n0010 - Content Type Image\n\n\n0x12\n\n\n\n\n\n\nContent-ID-Image \nPCF\n\n\n0001\n\n\n0011 - Content Type Image + \nPCF\n\n\n0x13\n\n\n\n\n\n\nContent-ID-Audio\n\n\n0001\n\n\n0100 - Content Type Audio\n\n\n0x14\n\n\n\n\n\n\nContent-ID-Audio \nPCF\n\n\n0001\n\n\n0101 - Content Type Audio + \nPCF\n\n\n0x15\n\n\n\n\n\n\nContent-ID-Video\n\n\n0001\n\n\n0110 - Content Type Video\n\n\n0x16\n\n\n\n\n\n\nContent-ID-Video \nPCF\n\n\n0001\n\n\n0111 - Content Type Video + \nPCF\n\n\n0x17\n\n\n\n\n\n\nContent-ID-Mixed\n\n\n0001\n\n\n1000 - Content Type Mixed\n\n\n0x18\n\n\n\n\n\n\nContent-ID Mixed \nPCF\n\n\n0001\n\n\n1001 - Content Type Mixed + \nPCF\n\n\n0x19\n\n\n\n\n\n\nData-ID\n\n\n0010\n\n\n0000 - Reserved\n\n\n0x20\n\n\n\n\n\n\nInstance-ID\n\n\n0011\n\n\n0000 - Reserved\n\n\n0x30\n\n\n\n\n\n\n\n\nThe body section of each component is specific to the component and always 8-bytes and can thus be fit into a 64-bit integer for efficient data processing. The following sections give an overview of how the differen components work and how they are generated.\n\n\nMeta-ID Component\n#\n\n\nThe Meta-ID component starts with a 1-byte header \n00000000\n. The first nibble \n0000\n indicates that this is a Meta-ID component type. The second nibble \n0000\n indicates that it belongs to an \nISCC\n of version 1. All subsequent components are expected to follow the specification of a version 1 \nISCC\n.\n\n\nThe Meta-ID body is built from a 64-bit \nsimilarity_hash\n over 4-\ncharacter\n n-grams of the basic metadata of the content to be identified.  The basic metadata supplied to the META-ID generating function is assumed to be UTF-8 encoded. Errors that occur during the decoding of such a bytestring input to a native Unicode MUST terminate the process and must not be silenced. An \nISCC\n generating application MUST provide a \nmeta_id\n function that accepts minimal and generic metadata and returns a \nBase58-\nISCC\n encoded\n Meta-ID component and trimmed metadata.\n\n\nInputs to Meta-ID function\n#\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nRequired\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntitle\n\n\ntext\n\n\nYes\n\n\nThe title of an intangible creation.\n\n\n\n\n\n\nextra\n\n\ntext\n\n\nNo\n\n\nAn optional short statement that distinguishes this intangible creation from another one for the purpose of Meta-ID uniqueness. (default: empty string)\n\n\n\n\n\n\nversion\n\n\ninteger\n\n\nNo\n\n\nISCC\n version number. (default: 0)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe basic metadata inputs are intentionally simple and generic. We abstain from more specific metadata for Meta-ID generation in favor of compatibility accross industries. Imagine a \ncreators\n input-field for metadata. Who would you list as the creators of a movie? The directors, writers the main actors? Would you list some of them or if not how do you decide whom you will list. All disambiguation of similar title data can be acomplished with the extra-field. Industry- and application-specific metadata requirements can be supplied as extended metadata with \nISCC\n registration.\n\n\n\n\nGenerate Meta-ID\n#\n\n\nAn \nISCC\n generating application must follow these steps in the given order to produce a stable Meta-ID:\n\n\n\n\nVerify the requested \nISCC\n version is supported by your implementation.\n\n\nApply \ntext_pre_normalize\n separately to the  \ntitle\n and \nextra\n inputs.\n\n\nApply \ntext_trim\n to the results of step 1. \nThe results of this step MUST be supplied as basic metadata for \nISCC\n registration.\n\n\nConcatenate trimmed\ntitle\n and \nextra\n from using a space ( \n\\u0020\n) as a seperator.\n\n\nApply \ntext_normalize\n to the results of step 3.\n\n\nCreate a list of 4 \ncharacter\n \nn-grams\n by sliding \ncharacter\n-wise through the result of step 4.\n\n\nEncode each n-gram from step 5 to an UTF-8 bytestring and calculate its \nxxHash64\n digest.\n\n\nApply \nsimilarity_hash\n to the list of digests from step 6.\n\n\nPrepend the 1-byte component header according to component type and \nISCC\n version (e.g. \n0x00\n) to the results of step 7.\n\n\nEncode the resulting 9 byte sequence with \nencode\n\n\nReturn encoded Meta-ID, trimmed \ntitle\n and trimmed \nextra\n data.\n\n\n\n\nSee also: \nMeta-ID reference code\n\n\n\n\nText trimming\n\n\nWhen trimming text be sure to trim the byte-length of the UTF-8 encoded version and not the number of characters. The trim point MUST be such, that it does not cut into multibyte characters. Characters might have different UTF-8 byte-length. For example \n\u00fc\n is 2-bytes, \n\u9a69\n is 3-bytes and \n\ud841\udf0e\n is 4-bytes. So the trimmed version of a string with 128 \n\u9a69\n-characters will result in a 42-\ncharacter\n string with a 126-byte UTF-8 encoded length. This is necessary because the results of this operation will be stored as basic metadata with strict byte size limits on the blockchain. \n\n\n\n\n\n\nAutomated Data-Ingestion\n\n\nApplications that perform automated data-ingestion SHOULD apply a custimized preliminary normalization to title data tailored to the dataset. Depending on catalog data removing pairs of brackets [], (), {}, and text inbetween them or cutting all text after the first occurence of a semicolon (;) or colon (:) can vastly improve de-duplication. \n\n\n\n\nDealing with Meta-ID collisions\n#\n\n\nIdeally we want multiple ISCCs that identify different manifestations of the \nsame intangible creation\n to be automatically grouped by an identical leading Meta-ID component. We call such a natural grouping an \nintended component collision\n. Metadata, captured and edited by humans, is notoriously unreliable. By using normalization and a similarity hash on the metadata we account for some of this variation while keeping the Meta-ID component somewhat stable. \n\n\nAuto-generated Meta-IDs components are \nexpected\n to miss some intended collisions. An application SHOULD check for such \nmissed intended component collisions\n before registering a new Meta-ID with the \ncanonical registry\n of ISCCs by conducting a similarity search and asking for user feedback.\n\n\nBut what about \nunintended component collisions\n? Such collisions might happen because two \ndifferent intangible creations\n have very similar or even identical metadata. But they might also happen simply by chance. With 2^56 possibile Meta-ID components the probability of random collisions rises in an S-cuved shape with the number of deployed ISCCs (see: \nHash Collision Probabilities\n).  We should keep in mind that, the Meta-ID component is only one part of a fully qualified \nISCC Code\n. Unintended collisions of the Meta-ID component are generally deemed as \nacceptable and expected\n. \n\n\nIf for any reason an application wants to avoid unintended collisions with pre-existing Meta-ID components it may utilze the \nextra\n-field. An application MUST first generate a Meta-ID without asking the user for input to the \nextra\n-field and then first check for collisions with the \ncanonical registry\n of ISCCs. After it finds a collision with a pre-existing Meta-ID it may display the metadata of the colliding entry and interact with the user to determine if it indeed is an unintended collision. Only if the user indicates an unintended collision, may the application ask for a disambiguation that is than added as an ammendment to the metadata via the \nextra\n-field to create a different Meta-ID component. The application may repeat the pre-existence check until it finds no collision or a user intended collision. The application MUST NOT supply autogenerated input to the \nextra\n-field.\n\n\nIt is our opinion that the concept of \nintended collisions\n of Meta-ID components is generally usefull concept and a net positive. But one must be aware that this characteristic also has its pitfalls. It is by no means an attempt to provide an unambigous - agreed upon - definition of \n\"identical intangible creations\"\n.\n\n\nContent-ID Component\n#\n\n\nThe Content-ID component has multiple subtypes. The subtypes correspond with the \nGeneric Media Types (\nGMT\n)\n. A fully qualified \nISCC\n can only have one Content-ID component of one specific \nGMT\n, but there may be multiple ISCCs with different Content-ID types per digital media object.\n\n\nA Content-ID is generated in two broad steps. In the first step, we extract and convert content from a rich media type to a normalized \nGMT\n. In the second step, we use a \nGMT\n-specific process to generate the Content-ID component of an \nISCC\n. \n\n\nGeneric Media Types\n#\n\n\nThe  Content-ID type is signaled by the first 3 bits of the second nibble of the first byte of the Content-ID:\n\n\n\n\n\n\n\n\nConent-ID Type\n\n\nNibble 2 Bits 0-3\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntext\n\n\n000\n\n\nGenerated from extracted and normalized plain-text\n\n\n\n\n\n\nimage\n\n\n001\n\n\nGenerated from normalized gray-scale pixel data\n\n\n\n\n\n\naudio\n\n\n010\n\n\nTo be defined in later version of specification\n\n\n\n\n\n\nvideo\n\n\n011\n\n\nTo be defined in later version of specification\n\n\n\n\n\n\nmixed\n\n\n100\n\n\nGenerated from multiplde Content-IDs\n\n\n\n\n\n\n\n\n101, 110, 111\n\n\nReserved for future versions of specification\n\n\n\n\n\n\n\n\nContent-ID-Text\n#\n\n\nThe Content-ID-Text is built from the extracted plain-text content of an encoded media object. To build a stable Content-ID-Text the plain-text content must first be extracted from the digital media object. It should be extracted in a way that is reproducible. There are many different text document formats out in the wilde and extracting plain-text from all of them is anything but a trivial task. While text-extraction is out of scope for this specification it is RECOMMENDED, that plain-text content SHOULD be extracted with the open-source \nApache Tika v1.17\n toolkit, if a generic reproducibility of the Content-ID-Text component is desired. \n\n\nAn \nISCC\n generating application MUST provide a \ncontent_id(text, partial=False)\n function that accepts UTF-8 encoded plain text and a boolean indicating the \npartial content flag\n as input and returns a Content-ID with \nGMT\n type \ntext\n. The procedure to create a Content-ID-Text is as follows:\n\n\n\n\nApply \ntext_pre_normalize\n.\n\n\nApply \ntext_normalize\n to the text input.\n\n\nSplit the normalized text into a list of words at whitespace boundaries.\n\n\nCreate a list of 5 word shingles by sliding word-wise through the list of words.\n\n\nCreate  a list of 32-bit unsigned integer features by applying \nxxHash32\n to results of step 4.\n\n\nApply \nminimum_hash\n to the list of features from step 5.\n\n\nCollect the least significant bits from the 128 MinHash features from step 6.\n\n\nCreate two 64-bit digests from the first and second half of the collected bits.\n\n\nApply \nsimilarity_hash\n to the digests returned from step 8.\n\n\nPrepend the 1-byte component header (\n0x10\n full content or \n0x11\n partial content).\n\n\nEncode and return the resulting 9-byte sequence with \nencode\n.\n\n\n\n\nSee also: \nContent-ID-Text reference code\n\n\nContent-ID-Image\n#\n\n\nFor the Content-ID-Image we are opting for a DCT-based perceptual image hash instead of a more sophisticated keypoint detection based method. In view of the generic deployabiility of the \nISCC\n we chose an algorithm that has moderate computation requirements and is easy to implement while still being robust against common image manipulations. \n\n\nAn \nISCC\n generating application MUST provide a \ncontent_id_image(image, partial=False)\n function that accepts a local file path to an image and returns a Content-ID with \nGMT\n type \nimage\n. The procedure to create a Content-ID-Image is as follows:\n\n\n\n\nApply \nimage_normalize\n to receive a two-dimensional array of grayscale pixel data.\n\n\nApply \nimage_hash\n to the results of step 1.\n\n\nPrepend the 1-byte component header (\n0x12\n full content or \n0x13\n partial content) to results of step 2.\n\n\nEncode and return the resulting 9-byte sequence with \nencode\n\n\n\n\nSee also: \nContent-ID-Image reference code\n\n\n\n\nImage Data Input\n\n\nThe \ncontent_id_image\n function may optionally accept the raw byte data of an encoded image or an internal native image object as input for convenience.\n\n\n\n\nContent-ID-Mixed\n#\n\n\nThe Content-ID-Mixed aggregates multiple Content-IDs of the same or different types. It may be used for digital media objects that embed multiples types of media or for collections of contents of the same type. First we have to collect contents from the mixed media object or content collection and generate Content-IDs for each item. An \nISCC\n conforming application must provide a \ncontent_id_mixed\n function that takes a list of Content-ID Codes as input and retuns a Content-ID-Mixed. Follow these steps to create a Content-ID-Mixed:\n\n\nSignature: \nconent_id_mixed(cids: List[str], partial: bool=False) -> str\n\n\n\n\nDecode the list of Content-IDs.\n\n\nExtract the \nfirst 8-bytes\n from each digest (\nNote\n: this includes the header part of the Content-IDs).\n\n\nApply \nsimilarity_hash\n to the list of digests from step 2.\n\n\nPrepend the 1-byte component header(\n0x18\n full content or \n0x19\n partial content)\n\n\nApply \nencode\n to the result of step 5 and return the result.\n\n\n\n\nSee also: \nContent-ID-Mixed reference code\n (LINKME)\n\n\nPartial Content Flag (\nPCF\n)\n#\n\n\nThe last bit of the header byte of the Content-ID is the \"Partial Content Flag\". It designates if the Content-ID applies to the full content or just some part of it. The \nPCF\n MUST be set as a \n0\n-bit (\nfull \nGMT\n-specific content\n) by default. Setting the \nPCF\n to \n1\n enables applications to create multiple linked ISCCs of partial extracts of a content collection. The exact semantics of \npartial content\n are outside of the scope of this specification. Applications that plan to support partial Content-IDs MUST clearly define their semantics.\n\n\n\n\n\n\nPCF\n Linking Example\n\n\nLet\u00b4s assume we have a single newspaper issue \"The Times - 03 Jan 2009\". You would generate one Meta-ID component with title \"The Times\" and extra \"03 Jan 2009\". The resulting Meta-ID component will be the grouping prefix in this szenario.\n\n\nWe use a Content-ID-Mixed with \nPCF\n \n0\n (not partial) for the \nISCC\n of the newspaper issue. We generate Data-ID and Instance-ID from the print PDF of the newspaper issue.\n\n\nTo create an \nISCC\n for a single extracted image that should convey context with the newspaper issue we reuse the Meta-ID of the newspaper issue and create a Content-ID-Image with \nPCF\n \n1\n (partial to the newspaper issue). For the Data-ID or Instance-ID of the image we are free to choose if we re-use those of the newspaper issue or create separate ones. The former would express strong specialization of the image to the newspaper issue (not likely to be usefull out of context). The latter would create a stronger link to an eventual standalone \nISCC\n of the image. Note that in any case the \nISCC\n of the individual image retains links in both ways:\n\n\n\n\nImage is linked to the newspaper issue by identical Meta-ID component\n\n\nImage is linked to the standalone version of the image by identical Content-ID-Image body \n\n\n\n\nThis is just one example that illustrates the flexibility that the \nPCF\n-Flag provides in concert with a grouping Meta-ID. With great flexibility comes great danger of complexity. Applications SHOULD do carefull planning before using the \nPCF\n-Flag with internally defined semantics.\n\n\n\n\nData-ID Component\n#\n\n\nFor the Data-ID that encodes data similarty we use a content defined chunking algorithm that provides some shift resistance and calculate the MinHash from those chunks. To accomodate for small files the first 100 chunks have a ~140-byte size target while the remaining chunks target ~ 6kb in size.\n\n\nThe Data-ID is built from the raw encoded data of the content to be identified. An \nISCC\n generating application MUST provide a \ndata_id\n function that accepts the raw encoded data as input.\n\n\nGenerate Data-ID\n#\n\n\n\n\nApply \ndata_chunks\n to the raw encoded content data.\n\n\nFor each chunk calculate the xxHash32 integer hash.\n\n\nApply \nminimum_hash\n to the resulting list of 32-bit unsigned integers.\n\n\nCollect the least significant bits from the 128 MinHash features.\n\n\nCreate two 64-bit digests from the first and second half of the collected bits.\n\n\nApply \nsimilarity_hash\n to the results of step 5.\n\n\nPrepend the 1-byte component header (e.g. 0x20).\n\n\nApply \nencode\n to the result of step 5 and return the result.\n\n\n\n\nSee also: \nData-ID reference code\n\n\nInstance-ID Component\n#\n\n\nThe Instance-ID is built from the raw data of the media object to be identified and serves as checksum for the media object. The raw data of the media object is split into 64-kB data-chunks. Then we build a hash-tree from those chunks and use the truncated \ntophash\n (merkle root) as component body of the Instance-ID.\n\n\nTo guard against length-extension attacks and second pre-image attacks we use double sha256 for hashing. We also prefix the hash input data with a \n0x00\n-byte for the leaf nodes hashes and with a \n0x01\n-byte for the  internal node hashes. While the Instance-ID itself is a non-cryptographic checksum, the full \ntophash\n may be supplied in the extended metadata of an \nISCC\n secure integrity verification is required.\n\n\n\n\nAn \nISCC\n generating application MUST provide a \ninstance_id\n function that accepts the raw data file as input and returns an encoded Instance-ID and a full hex-encoded 256-bit \ntophash\n. \n\n\nGenerate Instance-ID\n#\n\n\n\n\nSplit the raw bytes of the encoded media object into 64-kB chunks.\n\n\nFor each chunk calculate the \nsha256d\n of the concatenation of a \n0x00\n-byte and the chunk bytes. We call the resulting values \nleaf node hashes\n (\nLNH\n).\n\n\nCalculate the next level of the hash tree by applying \nsha256d\n to the concatenation of a \n0x01\n-byte and adjacent pairs of \nLNH\n values. If the length of the list of \nLNH\n values is uneven concatenate the last \nLNH\n value with itself. We call the resulting values \ninternal node hashes\n (\nINH\n).\n\n\nRecursively apply \n0x01\n-prefixed pairwise hashing to the results of  step 3 until the process yields only one hash value. We call this value the \ntophash\n.\n\n\nTrim the resulting \ntophash\n to the first 8 bytes.\n\n\nPrepend the 1-byte component header (e.g. \n0x30\n).\n\n\nEncode resulting 9-byte sequence with \nencode\n to an Instance-ID Code\n\n\nHex-Encode the \ntophash\n\n\nReturn the Intance-ID and the hex-encoded \ntophash\n\n\n\n\nSee also: \nInstance-ID reference code\n \n\n\nApplications may carry, store, and process the leaf node hashes for advanced streaming data identification or partial data integrity verification.\n\n\nISCC\n Metadata\n#\n\n\nAs a generic content identifier the \nISCC\n makes minimal assumptions about metadata that must or should be supplied together with an \nISCC\n. The RECOMMENDED data-interchange format for \nISCC\n metadata is \nJSON\n. We distinguish between \nBasic Metadata\n and \nExtended Metadata\n:\n\n\nBasic Metadata\n#\n\n\nBasic metadata for an \nISCC\n is metadata that is explicitly defined by this specification. The following table enumarates basic metadata fields for use in the top-level of the JSON metadata object:\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nRequired\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nversion\n\n\ninteger\n\n\nNo\n\n\nVersion of \nISCC\n Specification. Assumed to be 1 if omitted.\n\n\n\n\n\n\ntitle\n\n\ntext\n\n\nYes\n\n\nThe title of an intangible creation identified by the \nISCC\n. The normalized and trimmed UTF-8 encoded text MUST not exceed 128 Bytes. The result of processing \ntitle\n and \nextra\n data with the \nmeta_id\n function MUST  match the Meta-ID component of the \nISCC\n.\n\n\n\n\n\n\nextra\n\n\ntext\n\n\nNo\n\n\nAn optional short statement that distinguishes this intangible creation from another one for the purpose of Meta-ID uniqueness.\n\n\n\n\n\n\ntophash\n\n\ntext (hex)\n\n\nNo\n\n\nThe full hex-encoded \ntophash\n (merkle root) retuned by the \ninstance_id\n  function.\n\n\n\n\n\n\nmeta\n\n\narray\n\n\nNo\n\n\nA list of one or more \nextended metadata\n entries. Must include at least one entry if specified.\n\n\n\n\n\n\n\n\n\n\nAttention\n\n\nDepending on adoption and real world use, future versions of this specification may define new basic metadata fields. Applications MAY add custom fields at the top level of the JSON object but MUST prefix those fields with an underscore to avoid collisions with future extensions of this specification.\n\n\n\n\nExtended Metadata\n#\n\n\nExtended metadata for an \nISCC\n is metadata that is not explixitly defined by this specification. All such metadata SHOULD be supplied as JSON objects within the top-level \nmeta\narray field. This allows for a flexible and extendable way to supply additional industry specific metadata about the identified content. \n\n\nExtended metadata entries MUST be wrapped in JSON object of the following structure:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nschema\n\n\nThe \nschema\n-field may indicate a well known metadata schema (such as Dublin Core, IPTC, ID3v2, ONIX) that is used. RECOMMENDED \nschema\n: \"\nschema.org\n\"\n\n\n\n\n\n\nmediatype\n\n\nThe \nmediatype\n-field specifies an \nIANA Media Type\n. RECOMMENDED \nmediatype\n: \"application/ld+json\"\n\n\n\n\n\n\nurl\n\n\nAn URL that is expected to host the metadata with the indicated \nschema\n and \nmediatype\n. This field is only required if the \ndata\n-field is omitted.\n\n\n\n\n\n\ndata\n\n\nThe \ndata\n-field holds the metadata conforming to the indicated \nschema\n and \nmediatype.\n It is only required if the\nurl\n field is omitted.\n\n\n\n\n\n\n\n\nISCC\n Registration\n#\n\n\nThe \nISCC\n is a decentralized identifier. ISCCs can be generated for content by anybody who has access to the content. Due to the clustering properties of its components the \nISCC\n provides utility in data interchange and de-duplication scenarios even without a global registry. There is no central authority for the registration of \nISCC\n identifiers or certification of content authorship.\n\n\nAs an open system the \nISCC\n allows any person or organization to offer \nISCC\n registration services as they see fit and without the need to ask anyone for permission. This also presumes that no person or organization may claim exclusive authority about \nISCC\n registration.\n\n\nBlockchain Registry\n#\n\n\nA well known, decentralized, open, and public registry for canonical discoverability of \nISCC\n identified content is of great value. For this reason it is RECOMMENDED to register \nISCC\n identifiers on the open \niscc\n data-stream of the \nContent Blockchain\n. For details please refer to the \nISCC\n-Stream specification\n of the Content Blockchain.\n\n\nISCC\n Embedding\n#\n\n\nEmbedding \nISCC\n codes into content is only RECOMMENDED if it does not ceate a side effect. We call it a side effect if embedding an \nISCC\n code modifies the content to such an extent, that it yields a different \nISCC\n code.\n\n\nSide effects will depend on the combination of \nISCC\n components that are to be embedded. A Meta-ID can always be embedded without side effect because it does not depend on the content itself. Content-ID and Data-ID may not change if embedded in larger media objects. Instance-IDs cannot easily be embedded as they will inevitably have a side effect on the post-embedding Instance-ID without special processing.\n\n\nApplications MAY embed \nISCC\n codes that have side effects if they specify a procedure by which the embedded \nISCC\n codes can be stripped in such a way that the stripped content will yield the original embedded \nISCC\n codes.\n\n\n\n\nISCC\n Embedding\n\n\nWe are able to embed the following combination of components from the \nmarkdown version\n of this document into the document itself because adding or removing them has no side effect:\n\n\nISCC\n: 11VkJQj3dPoPN-1HUg6wnerR9jP\n\n\n\n\nISCC\n URI Scheme\n#\n\n\nThe purpose of the \nISCC\n URI scheme based on \nRFC 3986\n is to enable users to easily discover information like metadata or license offerings about a \nISCC\n marked content by simply clicking a link on a webpage or by scanning a QR-Code. \n\n\nThe scheme name is \niscc\n. The path component MUST be a fully qualified \nISCC Code\n without hyphens. An optional \nstream\n query key MAY indicate the blockchain stream information source. If the \nstream\n query key is omitted applications SHOULD return information from the open \nISCC\n Stream\n.\n\n\nThe scheme name component (\"iscc:\") is case-insensitive. Applications MUST accept any combination of uppercase and lowercase leters in the scheme name. All other URI components are case-sensitive.\n\n\nApplications MAY register themselves as handler for the \"iscc:\" URI scheme if no other handler is already registered. If another handler is already registered an application MAY ask the user to change it on the first run of the application.\n\n\nURI Syntax\n#\n\n\n<foo>\n means placeholder, \n[bar]\n means optional.\n\n\niscc:<fq-iscc-code>[?stream=<name>]\n\n\n\n\n\nURI Example\n#\n\n\niscc:11TcMGvUSzqoM1CqVA3ykFawyh1R1sH4Bz8A1of1d2Ju4VjWt26S?stream=smart-license\n\n\n\n\n\nProcedures & Algorithms\n#\n\n\nBase58-\nISCC\n#\n\n\nThe \nISCC\n uses a custom per-component data encoding similar to the \nzbase62\n encoding by \nZooko Wilcox-O'Hearn\n but with a 58-symbol table. The encoding does not require padding and will always yield component codes of 13 characters length for ths 72-bit component digests. The predictable size of the encoding is a property that allows for easy composition and decomposition of components without having to rely on a delimiter (hyphen) in the \nISCC\n code representation. Colliding body segments of the digest are preserved by encoding the header and body separately. The ASCII symbol table also minimizes transcription and OCR errors by omitting the easily confused characters \n'O', '0', 'I', 'l'\n.\n\n\nencode\n#\n\n\nSignature: \nencode(digest: bytes) -> str\n\n\nThe \nencode\n function accepts a 9-byte \nISCC\n Component Digest\n and returns the Base58-\nISCC\n encoded  alphanumeric string of 13 characters which we call the \nISCC\n-Component Code\n.\n\n\nSee also: \nBase-\nISCC\n Encoding reference code\n\n\ndecode\n#\n\n\nSignature: decode(code: str) -> bytes\n\n\nthe \ndecode\n function accepts a 13-\ncharacter\n \nISCC\n-Component Code\n and returns the corresponding 9-byte \nISCC\n-Component Digest\n.\n\n\nSee also: \nBase-\nISCC\n Decoding reference code\n\n\nContent Normalization\n#\n\n\nThe \nISCC\n standardizes some content normalization procedures to support reproducible and stable identifiers. Following the list of normalization functions that MUST be provided by a conforming implementation.\n\n\ntext_pre_normalize\n#\n\n\nSignature: \ntext_pre_normalize(text: str|bytes) -> str\n\n\nDecodes raw plain-text data and applies Unicode \nNormalization Form KC (NFKC)\n . The plain-text data MUST be stripped of any markup beforhand. Text input is expected to be UTF-8 encoded plain-text data or a native type of the implementig programming language that supports Unicode. Text decoding errors MUST fail with an error.\n\n\nSee also: \nText pre-normalization reference code\n\n\ntext_trim\n#\n\n\nSignature: \ntext_trim(text: str) -> str\n\n\nTrim text such that its UTF-8 encoded byte representation does not exceed 128-bytes each.\n\n\nSee also: \nText trimming reference code\n\n\ntext_normalize\n#\n\n\nSignature: \ntext_normalize(text: str) -> str\n\n\nWe define a text normalization function that is specific to our application. It takes unicode text as an input and returns \nnormalized\n Unicode text for further algorithmic processing. The \ntext_normalize\n function performs the following operations in the given order while each step works with the results of the previous operation:\n\n\n\n\nDecompose the input text by applying \nUnicode Normalization Form D (NFD)\n.\n\n\nFilter and normalize text by iterating over unicode characters while:\n\n\nreplacing groups of one or more consecutive \nSeparator\n characters (\nUnicode categories\n Zs, Zl and Zp) with exactly one Unicode \nSPACE\n \ncharacter\n (\nU+0020\n) .\n\n\nremoving characters that are not in one of the Unicode categories \nSeparator\n , \nLetter\n, \nNumber\n or \nSymbol\n.\n\n\nconverting characters to lower case.\n\n\nRemove any leading or trailing \nSeparator\n characters.\n\n\nRe-Compose the text by applying \nUnicode Normalization Form C (NFC)\n.\n\n\n\n\nSee also: \nText normalization reference code\n\n\nimage_normalize\n#\n\n\nSignature: \nimage_normalize(img) -> List[List[int]]\n\n\nAccepts a file path, byte-stream or raw binary image data and MUST at least support JPEG, PNG, and GIF image formats. Normalize the image with the following steps:\n\n\n\n\nConvertf the image to greyscale\n\n\nResize the image to 32x32 pixels using \nbicubic interpolation\n\n\nCreate a 32x32 two-dimensional array of 8-bit grayscale values from the image data\n\n\n\n\nSee also: \nImage normalization reference code\n\n\nFeature Hashing\n#\n\n\nThe \nISCC\n standardizes various feature hashing algorithms that reduce content features to a binary vector used as the body of the various Content-ID components.\n\n\nsimilarity_hash\n#\n\n\nSignature: \nsimilarity_hash(hash_digests: Sequenc[ByteString]) -> bytes\n\n\nThe \nsimilarity_hash\n function takes a sequence of hash digests which represent a set of features. Each of the digests MUST be of equal size. The function returns a new hash digest (raw 8-bit bytes) of the same size. For each bit in the input hashes calulate the number of hashes with that bit set and substract the the count of hashes where it is not set. For the output hash set the same bit position to \n0\n if the count is negative or \n1\n if it is zero or positive. The resulting hash digest will retain similarity for similar sets of input hashes. See also  \n[Charikar2002]\n.\n\n\n\n\nSee also: \nSimilarity hash reference code\n\n\nminimum_hash\n#\n\n\nSignature: \nminimum_hash(features: Iterable[int]) -> List[int]\n\n\nThe \nminimum_hash\n function takes an arbitrary sized set of 32-bit integer features and reduces it to a fixed size vector of 128 features such that it preserves similarity with other sets. It is based on the MinHash implementation of the \ndatasketch\n library by \nEric Zhu\n.\n\n\nSee also: \nMinimum hash reference code\n\n\nimage_hash\n#\n\n\nSignature: \nimage_hash(pixels: List[List[int]]) -> bytes\n\n\n\n\nPerform a discrete cosine transform per row of input pixels.\n\n\nPerform a discrete cosine transform per column on the resulting matrix from step 2.\n\n\nExtract upper left 8x8 corner of array from step 2 as a flat list.\n\n\nCalculate the median of the results from step 3.\n\n\nCreate a 64-bit digest by iterating over the values of step 5 and setting a  \n1\n- for values above median and \n0\n for values below or equal to median.\n\n\nReturn results from step 5.\n\n\n\n\nSee also: \nImage hash reference code\n\n\nContent Defined Chunking\n#\n\n\nFor shift resistant data chunking the \nISCC\n requires a custom chunking algorithm:\n\n\ndata_chunks\n#\n\n\nSignature: \ndata_chunks(data: stream) -> Iterator[bytes]\n\n\nThe \ndata_chunks\n function accepts a byte-stream and returns variable sized chunks. Chunk boundaries are determined by a gear based chunking algorithm based on \n[WenXia2016]\n.\n\n\nSee also: \nCDC\n reference code\n\n\nConformance Testing\n#\n\n\nAn application that claims \nISCC\n conformance MUST pass the \nISCC\n conformance test suite. The test suite is available as json data in our \nGithub Repository\n. Testdata is stuctured as follows:\n\n\n{\n\n    \n\"<function_name>\"\n:\n \n{\n\n        \n\"<test_name>\"\n:\n \n{\n\n            \n\"inputs\"\n:\n \n[\n\"<value1>\"\n,\n \n\"<value2>\"\n],\n\n            \n\"outputs\"\n:\n \n[\n\"value1>\"\n,\n \n\"<value2>\"\n]\n\n        \n}\n\n    \n}\n\n\n}",
            "title": "Specification"
        },
        {
            "location": "/specification/#iscc-specification-v098",
            "text": "Attention  This document is a work in progress draft! It may be updated, replaced, or obsoleted by other documents at any time. This document MUST not be used as reference material or cited other than as \"work in progress\".",
            "title": "ISCC - Specification v0.9.8"
        },
        {
            "location": "/specification/#abstract",
            "text": "The  International Standard Content Code ( ISCC ) , is an open and decentralized digital media identifier. An  ISCC  can be created from digital content and its basic metadata by anybody who follows the procedures of the  ISCC  specification or by using open source software that supports  ISCC  creation  conforming to the  ISCC  specification .",
            "title": "Abstract"
        },
        {
            "location": "/specification/#note-to-readers",
            "text": "For public discussion of issues for this draft please use the Github issue tracker:  https://github.com/coblo/iscc-specs/issues .  The latest published version of this draft can be found at  http://iscc.codes/specification/ .   Public review, discussion and contributions are welcome.",
            "title": "Note to Readers"
        },
        {
            "location": "/specification/#about-this-document",
            "text": "This document proposes an open and vendor neutral  ISCC  standard and describes the technical procedures to create and manage  ISCC  identifiers. The first version of this document is produced as a prototype by the  Content Blockchain Project  and received funding from the  Google Digital News Initiative (DNI) . The content of this document is determined by its authors in an open and public consensus process.",
            "title": "About this Document"
        },
        {
            "location": "/specification/#conventions-and-terminology",
            "text": "The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in  [RFC 2119] .",
            "title": "Conventions and Terminology"
        },
        {
            "location": "/specification/#definitions",
            "text": "Basic Metadata:  Minimal set of metadata about the content that is identified by an  ISCC . This metadata may impact the derived Meta-ID Component  Character:  Throughout this specification a  character  is meant to be interpreted as one Unicode code point. This also means that due to the structure of Unicode a  character  is not necessarily a full glyph but might be a combining accent or similar.  Digital Media Object:  A blob of raw bytes with some media type specific encoding.  Extended Metadata:  Metadata that is not encoded within the  ISCC  Meta-ID but may be supplied together with the  ISCC .  Generic Media Type:  A basic content type such as plain text in a normalized and  generic  ( UTF-8 ) encoding format.  ISCC :  International Standard Content Code  ISCC Code :  The printable text encoded representation of an  ISCC  ISCC Digest :  The raw binary data of an  ISCC",
            "title": "Definitions"
        },
        {
            "location": "/specification/#introduction",
            "text": "An  ISCC  permanently identifies content at multiple levels of  granularity . It is algorithmically generated from basic metadata and the contents of a digital media object. It is designed for being registered and stored on a public and decentralized blockchain. An  ISCC  for a media object can be created and registered by the content author, a publisher, a service provider or anybody else. By itself the  ISCC  and its basic registration on a blockchain does not make any statement or claim about authorship or ownership of the identified content.",
            "title": "Introduction"
        },
        {
            "location": "/specification/#iscc-structure",
            "text": "A  Fully Qualified  ISCC Digest  is a fixed size sequence of  36 bytes (288 bits)  assembled from multiple sub-components. The  Fully Qualified   ISCC Code  is a  52  character  encoded printable string representation of a complete  ISCC Digest . This is a high-level overview of the  ISCC  creation process:",
            "title": "ISCC Structure"
        },
        {
            "location": "/specification/#iscc-components",
            "text": "The  ISCC Digest  is built from multiple self-describing 72-bit components:     Components:  Meta-ID  Content-ID  Data-ID  Instance-ID      Context:  Intangible creation  Content similarity  Data similarity  Data checksum    Input:  Metadata  Extracted  content  Raw data  Raw data    Algorithms:  Similarity Hash  Type specific  CDC , Minimum Hash  CDC , Hash Tree    Size:  72 bits  72 bits  72 bits  72 bits     ISCC  components MAY be used separately or in combination by applications for various purposes. Individual components MUST be presented as 13- character   base58-iscc  encoded strings to end users and MAY be prefixed with their component name.   Single component  ISCC -Code (13 characters)  Meta-ID : 11RDXN6ea57QT   Combinations of components MUST include the Meta-ID component and MUST be ordered as  Meta-ID ,  Content-ID ,  Data-ID , and  Instance-ID . Individual components MAY be skipped and SHOULD be separated with hyphens. A combination of components SHOULD be prefixed with \" ISCC \".   Combination of  ISCC -Code components  ISCC : 11RDXN6ea57QT-1HWMGKrLki1fL-1ZYcY8d3uTYj8   A  Fully Qualified  ISCC Code  is an ordered sequence of Meta-ID, Content-ID, Data-ID, and Instance-ID codes. It SHOULD be prefixed with  ISCC  and MAY be seperated by hypens.   Fully Qualified  ISCC -Code (52 characters)  ISCC : 11RDXN6ea57QT1HWMGKrLki1fL1ZYcY8d3uTYj81qYuuCMStgpjs    Fully Qualified  ISCC -Code with hyphens (55 characters)  ISCC : 11RDXN6ea57QT-1HWMGKrLki1fL-1ZYcY8d3uTYj8-1qYuuCMStgpjs",
            "title": "ISCC Components"
        },
        {
            "location": "/specification/#component-types",
            "text": "Each component has the same basic structure of a  1-byte header  and a  8-byte body  section.   The 1-byte header of each component is subdivided into 2 nibbles (4 bits). The first nibble specifies the component type while the second nibble is component specific.  The header only needs to be carried in the encoded representation. As similarity searches accross different components are of little use, the type information contained in the header of each component can be safely ignored after an  ISCC  has been decomposed and internaly typed by an application.",
            "title": "Component Types"
        },
        {
            "location": "/specification/#list-of-component-headers",
            "text": "Component  Nibble-1  Nibble-2  Byte      Meta-ID  0000  0000 -  ISCC  version 1  0x00    Content-ID-Text  0001  0000 - Content Type Text  0x10    Content-ID-Text  PCF  0001  0001 - Content Type Text  +  PCF  0x11    Content-ID-Image  0001  0010 - Content Type Image  0x12    Content-ID-Image  PCF  0001  0011 - Content Type Image +  PCF  0x13    Content-ID-Audio  0001  0100 - Content Type Audio  0x14    Content-ID-Audio  PCF  0001  0101 - Content Type Audio +  PCF  0x15    Content-ID-Video  0001  0110 - Content Type Video  0x16    Content-ID-Video  PCF  0001  0111 - Content Type Video +  PCF  0x17    Content-ID-Mixed  0001  1000 - Content Type Mixed  0x18    Content-ID Mixed  PCF  0001  1001 - Content Type Mixed +  PCF  0x19    Data-ID  0010  0000 - Reserved  0x20    Instance-ID  0011  0000 - Reserved  0x30     The body section of each component is specific to the component and always 8-bytes and can thus be fit into a 64-bit integer for efficient data processing. The following sections give an overview of how the differen components work and how they are generated.",
            "title": "List of Component Headers"
        },
        {
            "location": "/specification/#meta-id-component",
            "text": "The Meta-ID component starts with a 1-byte header  00000000 . The first nibble  0000  indicates that this is a Meta-ID component type. The second nibble  0000  indicates that it belongs to an  ISCC  of version 1. All subsequent components are expected to follow the specification of a version 1  ISCC .  The Meta-ID body is built from a 64-bit  similarity_hash  over 4- character  n-grams of the basic metadata of the content to be identified.  The basic metadata supplied to the META-ID generating function is assumed to be UTF-8 encoded. Errors that occur during the decoding of such a bytestring input to a native Unicode MUST terminate the process and must not be silenced. An  ISCC  generating application MUST provide a  meta_id  function that accepts minimal and generic metadata and returns a  Base58- ISCC  encoded  Meta-ID component and trimmed metadata.",
            "title": "Meta-ID Component"
        },
        {
            "location": "/specification/#inputs-to-meta-id-function",
            "text": "Name  Type  Required  Description      title  text  Yes  The title of an intangible creation.    extra  text  No  An optional short statement that distinguishes this intangible creation from another one for the purpose of Meta-ID uniqueness. (default: empty string)    version  integer  No  ISCC  version number. (default: 0)      Note  The basic metadata inputs are intentionally simple and generic. We abstain from more specific metadata for Meta-ID generation in favor of compatibility accross industries. Imagine a  creators  input-field for metadata. Who would you list as the creators of a movie? The directors, writers the main actors? Would you list some of them or if not how do you decide whom you will list. All disambiguation of similar title data can be acomplished with the extra-field. Industry- and application-specific metadata requirements can be supplied as extended metadata with  ISCC  registration.",
            "title": "Inputs to Meta-ID function"
        },
        {
            "location": "/specification/#generate-meta-id",
            "text": "An  ISCC  generating application must follow these steps in the given order to produce a stable Meta-ID:   Verify the requested  ISCC  version is supported by your implementation.  Apply  text_pre_normalize  separately to the   title  and  extra  inputs.  Apply  text_trim  to the results of step 1.  The results of this step MUST be supplied as basic metadata for  ISCC  registration.  Concatenate trimmed title  and  extra  from using a space (  \\u0020 ) as a seperator.  Apply  text_normalize  to the results of step 3.  Create a list of 4  character   n-grams  by sliding  character -wise through the result of step 4.  Encode each n-gram from step 5 to an UTF-8 bytestring and calculate its  xxHash64  digest.  Apply  similarity_hash  to the list of digests from step 6.  Prepend the 1-byte component header according to component type and  ISCC  version (e.g.  0x00 ) to the results of step 7.  Encode the resulting 9 byte sequence with  encode  Return encoded Meta-ID, trimmed  title  and trimmed  extra  data.   See also:  Meta-ID reference code   Text trimming  When trimming text be sure to trim the byte-length of the UTF-8 encoded version and not the number of characters. The trim point MUST be such, that it does not cut into multibyte characters. Characters might have different UTF-8 byte-length. For example  \u00fc  is 2-bytes,  \u9a69  is 3-bytes and  \ud841\udf0e  is 4-bytes. So the trimmed version of a string with 128  \u9a69 -characters will result in a 42- character  string with a 126-byte UTF-8 encoded length. This is necessary because the results of this operation will be stored as basic metadata with strict byte size limits on the blockchain.     Automated Data-Ingestion  Applications that perform automated data-ingestion SHOULD apply a custimized preliminary normalization to title data tailored to the dataset. Depending on catalog data removing pairs of brackets [], (), {}, and text inbetween them or cutting all text after the first occurence of a semicolon (;) or colon (:) can vastly improve de-duplication.",
            "title": "Generate Meta-ID"
        },
        {
            "location": "/specification/#dealing-with-meta-id-collisions",
            "text": "Ideally we want multiple ISCCs that identify different manifestations of the  same intangible creation  to be automatically grouped by an identical leading Meta-ID component. We call such a natural grouping an  intended component collision . Metadata, captured and edited by humans, is notoriously unreliable. By using normalization and a similarity hash on the metadata we account for some of this variation while keeping the Meta-ID component somewhat stable.   Auto-generated Meta-IDs components are  expected  to miss some intended collisions. An application SHOULD check for such  missed intended component collisions  before registering a new Meta-ID with the  canonical registry  of ISCCs by conducting a similarity search and asking for user feedback.  But what about  unintended component collisions ? Such collisions might happen because two  different intangible creations  have very similar or even identical metadata. But they might also happen simply by chance. With 2^56 possibile Meta-ID components the probability of random collisions rises in an S-cuved shape with the number of deployed ISCCs (see:  Hash Collision Probabilities ).  We should keep in mind that, the Meta-ID component is only one part of a fully qualified  ISCC Code . Unintended collisions of the Meta-ID component are generally deemed as  acceptable and expected .   If for any reason an application wants to avoid unintended collisions with pre-existing Meta-ID components it may utilze the  extra -field. An application MUST first generate a Meta-ID without asking the user for input to the  extra -field and then first check for collisions with the  canonical registry  of ISCCs. After it finds a collision with a pre-existing Meta-ID it may display the metadata of the colliding entry and interact with the user to determine if it indeed is an unintended collision. Only if the user indicates an unintended collision, may the application ask for a disambiguation that is than added as an ammendment to the metadata via the  extra -field to create a different Meta-ID component. The application may repeat the pre-existence check until it finds no collision or a user intended collision. The application MUST NOT supply autogenerated input to the  extra -field.  It is our opinion that the concept of  intended collisions  of Meta-ID components is generally usefull concept and a net positive. But one must be aware that this characteristic also has its pitfalls. It is by no means an attempt to provide an unambigous - agreed upon - definition of  \"identical intangible creations\" .",
            "title": "Dealing with Meta-ID collisions"
        },
        {
            "location": "/specification/#content-id-component",
            "text": "The Content-ID component has multiple subtypes. The subtypes correspond with the  Generic Media Types ( GMT ) . A fully qualified  ISCC  can only have one Content-ID component of one specific  GMT , but there may be multiple ISCCs with different Content-ID types per digital media object.  A Content-ID is generated in two broad steps. In the first step, we extract and convert content from a rich media type to a normalized  GMT . In the second step, we use a  GMT -specific process to generate the Content-ID component of an  ISCC .",
            "title": "Content-ID Component"
        },
        {
            "location": "/specification/#generic-media-types",
            "text": "The  Content-ID type is signaled by the first 3 bits of the second nibble of the first byte of the Content-ID:     Conent-ID Type  Nibble 2 Bits 0-3  Description      text  000  Generated from extracted and normalized plain-text    image  001  Generated from normalized gray-scale pixel data    audio  010  To be defined in later version of specification    video  011  To be defined in later version of specification    mixed  100  Generated from multiplde Content-IDs     101, 110, 111  Reserved for future versions of specification",
            "title": "Generic Media Types"
        },
        {
            "location": "/specification/#content-id-text",
            "text": "The Content-ID-Text is built from the extracted plain-text content of an encoded media object. To build a stable Content-ID-Text the plain-text content must first be extracted from the digital media object. It should be extracted in a way that is reproducible. There are many different text document formats out in the wilde and extracting plain-text from all of them is anything but a trivial task. While text-extraction is out of scope for this specification it is RECOMMENDED, that plain-text content SHOULD be extracted with the open-source  Apache Tika v1.17  toolkit, if a generic reproducibility of the Content-ID-Text component is desired.   An  ISCC  generating application MUST provide a  content_id(text, partial=False)  function that accepts UTF-8 encoded plain text and a boolean indicating the  partial content flag  as input and returns a Content-ID with  GMT  type  text . The procedure to create a Content-ID-Text is as follows:   Apply  text_pre_normalize .  Apply  text_normalize  to the text input.  Split the normalized text into a list of words at whitespace boundaries.  Create a list of 5 word shingles by sliding word-wise through the list of words.  Create  a list of 32-bit unsigned integer features by applying  xxHash32  to results of step 4.  Apply  minimum_hash  to the list of features from step 5.  Collect the least significant bits from the 128 MinHash features from step 6.  Create two 64-bit digests from the first and second half of the collected bits.  Apply  similarity_hash  to the digests returned from step 8.  Prepend the 1-byte component header ( 0x10  full content or  0x11  partial content).  Encode and return the resulting 9-byte sequence with  encode .   See also:  Content-ID-Text reference code",
            "title": "Content-ID-Text"
        },
        {
            "location": "/specification/#content-id-image",
            "text": "For the Content-ID-Image we are opting for a DCT-based perceptual image hash instead of a more sophisticated keypoint detection based method. In view of the generic deployabiility of the  ISCC  we chose an algorithm that has moderate computation requirements and is easy to implement while still being robust against common image manipulations.   An  ISCC  generating application MUST provide a  content_id_image(image, partial=False)  function that accepts a local file path to an image and returns a Content-ID with  GMT  type  image . The procedure to create a Content-ID-Image is as follows:   Apply  image_normalize  to receive a two-dimensional array of grayscale pixel data.  Apply  image_hash  to the results of step 1.  Prepend the 1-byte component header ( 0x12  full content or  0x13  partial content) to results of step 2.  Encode and return the resulting 9-byte sequence with  encode   See also:  Content-ID-Image reference code   Image Data Input  The  content_id_image  function may optionally accept the raw byte data of an encoded image or an internal native image object as input for convenience.",
            "title": "Content-ID-Image"
        },
        {
            "location": "/specification/#content-id-mixed",
            "text": "The Content-ID-Mixed aggregates multiple Content-IDs of the same or different types. It may be used for digital media objects that embed multiples types of media or for collections of contents of the same type. First we have to collect contents from the mixed media object or content collection and generate Content-IDs for each item. An  ISCC  conforming application must provide a  content_id_mixed  function that takes a list of Content-ID Codes as input and retuns a Content-ID-Mixed. Follow these steps to create a Content-ID-Mixed:  Signature:  conent_id_mixed(cids: List[str], partial: bool=False) -> str   Decode the list of Content-IDs.  Extract the  first 8-bytes  from each digest ( Note : this includes the header part of the Content-IDs).  Apply  similarity_hash  to the list of digests from step 2.  Prepend the 1-byte component header( 0x18  full content or  0x19  partial content)  Apply  encode  to the result of step 5 and return the result.   See also:  Content-ID-Mixed reference code  (LINKME)",
            "title": "Content-ID-Mixed"
        },
        {
            "location": "/specification/#partial-content-flag-pcf",
            "text": "The last bit of the header byte of the Content-ID is the \"Partial Content Flag\". It designates if the Content-ID applies to the full content or just some part of it. The  PCF  MUST be set as a  0 -bit ( full  GMT -specific content ) by default. Setting the  PCF  to  1  enables applications to create multiple linked ISCCs of partial extracts of a content collection. The exact semantics of  partial content  are outside of the scope of this specification. Applications that plan to support partial Content-IDs MUST clearly define their semantics.    PCF  Linking Example  Let\u00b4s assume we have a single newspaper issue \"The Times - 03 Jan 2009\". You would generate one Meta-ID component with title \"The Times\" and extra \"03 Jan 2009\". The resulting Meta-ID component will be the grouping prefix in this szenario.  We use a Content-ID-Mixed with  PCF   0  (not partial) for the  ISCC  of the newspaper issue. We generate Data-ID and Instance-ID from the print PDF of the newspaper issue.  To create an  ISCC  for a single extracted image that should convey context with the newspaper issue we reuse the Meta-ID of the newspaper issue and create a Content-ID-Image with  PCF   1  (partial to the newspaper issue). For the Data-ID or Instance-ID of the image we are free to choose if we re-use those of the newspaper issue or create separate ones. The former would express strong specialization of the image to the newspaper issue (not likely to be usefull out of context). The latter would create a stronger link to an eventual standalone  ISCC  of the image. Note that in any case the  ISCC  of the individual image retains links in both ways:   Image is linked to the newspaper issue by identical Meta-ID component  Image is linked to the standalone version of the image by identical Content-ID-Image body    This is just one example that illustrates the flexibility that the  PCF -Flag provides in concert with a grouping Meta-ID. With great flexibility comes great danger of complexity. Applications SHOULD do carefull planning before using the  PCF -Flag with internally defined semantics.",
            "title": "Partial Content Flag (PCF)"
        },
        {
            "location": "/specification/#data-id-component",
            "text": "For the Data-ID that encodes data similarty we use a content defined chunking algorithm that provides some shift resistance and calculate the MinHash from those chunks. To accomodate for small files the first 100 chunks have a ~140-byte size target while the remaining chunks target ~ 6kb in size.  The Data-ID is built from the raw encoded data of the content to be identified. An  ISCC  generating application MUST provide a  data_id  function that accepts the raw encoded data as input.",
            "title": "Data-ID Component"
        },
        {
            "location": "/specification/#generate-data-id",
            "text": "Apply  data_chunks  to the raw encoded content data.  For each chunk calculate the xxHash32 integer hash.  Apply  minimum_hash  to the resulting list of 32-bit unsigned integers.  Collect the least significant bits from the 128 MinHash features.  Create two 64-bit digests from the first and second half of the collected bits.  Apply  similarity_hash  to the results of step 5.  Prepend the 1-byte component header (e.g. 0x20).  Apply  encode  to the result of step 5 and return the result.   See also:  Data-ID reference code",
            "title": "Generate Data-ID"
        },
        {
            "location": "/specification/#instance-id-component",
            "text": "The Instance-ID is built from the raw data of the media object to be identified and serves as checksum for the media object. The raw data of the media object is split into 64-kB data-chunks. Then we build a hash-tree from those chunks and use the truncated  tophash  (merkle root) as component body of the Instance-ID.  To guard against length-extension attacks and second pre-image attacks we use double sha256 for hashing. We also prefix the hash input data with a  0x00 -byte for the leaf nodes hashes and with a  0x01 -byte for the  internal node hashes. While the Instance-ID itself is a non-cryptographic checksum, the full  tophash  may be supplied in the extended metadata of an  ISCC  secure integrity verification is required.   An  ISCC  generating application MUST provide a  instance_id  function that accepts the raw data file as input and returns an encoded Instance-ID and a full hex-encoded 256-bit  tophash .",
            "title": "Instance-ID Component"
        },
        {
            "location": "/specification/#generate-instance-id",
            "text": "Split the raw bytes of the encoded media object into 64-kB chunks.  For each chunk calculate the  sha256d  of the concatenation of a  0x00 -byte and the chunk bytes. We call the resulting values  leaf node hashes  ( LNH ).  Calculate the next level of the hash tree by applying  sha256d  to the concatenation of a  0x01 -byte and adjacent pairs of  LNH  values. If the length of the list of  LNH  values is uneven concatenate the last  LNH  value with itself. We call the resulting values  internal node hashes  ( INH ).  Recursively apply  0x01 -prefixed pairwise hashing to the results of  step 3 until the process yields only one hash value. We call this value the  tophash .  Trim the resulting  tophash  to the first 8 bytes.  Prepend the 1-byte component header (e.g.  0x30 ).  Encode resulting 9-byte sequence with  encode  to an Instance-ID Code  Hex-Encode the  tophash  Return the Intance-ID and the hex-encoded  tophash   See also:  Instance-ID reference code    Applications may carry, store, and process the leaf node hashes for advanced streaming data identification or partial data integrity verification.",
            "title": "Generate Instance-ID"
        },
        {
            "location": "/specification/#iscc-metadata",
            "text": "As a generic content identifier the  ISCC  makes minimal assumptions about metadata that must or should be supplied together with an  ISCC . The RECOMMENDED data-interchange format for  ISCC  metadata is  JSON . We distinguish between  Basic Metadata  and  Extended Metadata :",
            "title": "ISCC Metadata"
        },
        {
            "location": "/specification/#basic-metadata",
            "text": "Basic metadata for an  ISCC  is metadata that is explicitly defined by this specification. The following table enumarates basic metadata fields for use in the top-level of the JSON metadata object:     Name  Type  Required  Description      version  integer  No  Version of  ISCC  Specification. Assumed to be 1 if omitted.    title  text  Yes  The title of an intangible creation identified by the  ISCC . The normalized and trimmed UTF-8 encoded text MUST not exceed 128 Bytes. The result of processing  title  and  extra  data with the  meta_id  function MUST  match the Meta-ID component of the  ISCC .    extra  text  No  An optional short statement that distinguishes this intangible creation from another one for the purpose of Meta-ID uniqueness.    tophash  text (hex)  No  The full hex-encoded  tophash  (merkle root) retuned by the  instance_id   function.    meta  array  No  A list of one or more  extended metadata  entries. Must include at least one entry if specified.      Attention  Depending on adoption and real world use, future versions of this specification may define new basic metadata fields. Applications MAY add custom fields at the top level of the JSON object but MUST prefix those fields with an underscore to avoid collisions with future extensions of this specification.",
            "title": "Basic Metadata"
        },
        {
            "location": "/specification/#extended-metadata",
            "text": "Extended metadata for an  ISCC  is metadata that is not explixitly defined by this specification. All such metadata SHOULD be supplied as JSON objects within the top-level  meta array field. This allows for a flexible and extendable way to supply additional industry specific metadata about the identified content.   Extended metadata entries MUST be wrapped in JSON object of the following structure:     Name  Description      schema  The  schema -field may indicate a well known metadata schema (such as Dublin Core, IPTC, ID3v2, ONIX) that is used. RECOMMENDED  schema : \" schema.org \"    mediatype  The  mediatype -field specifies an  IANA Media Type . RECOMMENDED  mediatype : \"application/ld+json\"    url  An URL that is expected to host the metadata with the indicated  schema  and  mediatype . This field is only required if the  data -field is omitted.    data  The  data -field holds the metadata conforming to the indicated  schema  and  mediatype.  It is only required if the url  field is omitted.",
            "title": "Extended Metadata"
        },
        {
            "location": "/specification/#iscc-registration",
            "text": "The  ISCC  is a decentralized identifier. ISCCs can be generated for content by anybody who has access to the content. Due to the clustering properties of its components the  ISCC  provides utility in data interchange and de-duplication scenarios even without a global registry. There is no central authority for the registration of  ISCC  identifiers or certification of content authorship.  As an open system the  ISCC  allows any person or organization to offer  ISCC  registration services as they see fit and without the need to ask anyone for permission. This also presumes that no person or organization may claim exclusive authority about  ISCC  registration.",
            "title": "ISCC Registration"
        },
        {
            "location": "/specification/#blockchain-registry",
            "text": "A well known, decentralized, open, and public registry for canonical discoverability of  ISCC  identified content is of great value. For this reason it is RECOMMENDED to register  ISCC  identifiers on the open  iscc  data-stream of the  Content Blockchain . For details please refer to the  ISCC -Stream specification  of the Content Blockchain.",
            "title": "Blockchain Registry"
        },
        {
            "location": "/specification/#iscc-embedding",
            "text": "Embedding  ISCC  codes into content is only RECOMMENDED if it does not ceate a side effect. We call it a side effect if embedding an  ISCC  code modifies the content to such an extent, that it yields a different  ISCC  code.  Side effects will depend on the combination of  ISCC  components that are to be embedded. A Meta-ID can always be embedded without side effect because it does not depend on the content itself. Content-ID and Data-ID may not change if embedded in larger media objects. Instance-IDs cannot easily be embedded as they will inevitably have a side effect on the post-embedding Instance-ID without special processing.  Applications MAY embed  ISCC  codes that have side effects if they specify a procedure by which the embedded  ISCC  codes can be stripped in such a way that the stripped content will yield the original embedded  ISCC  codes.   ISCC  Embedding  We are able to embed the following combination of components from the  markdown version  of this document into the document itself because adding or removing them has no side effect:  ISCC : 11VkJQj3dPoPN-1HUg6wnerR9jP",
            "title": "ISCC Embedding"
        },
        {
            "location": "/specification/#iscc-uri-scheme",
            "text": "The purpose of the  ISCC  URI scheme based on  RFC 3986  is to enable users to easily discover information like metadata or license offerings about a  ISCC  marked content by simply clicking a link on a webpage or by scanning a QR-Code.   The scheme name is  iscc . The path component MUST be a fully qualified  ISCC Code  without hyphens. An optional  stream  query key MAY indicate the blockchain stream information source. If the  stream  query key is omitted applications SHOULD return information from the open  ISCC  Stream .  The scheme name component (\"iscc:\") is case-insensitive. Applications MUST accept any combination of uppercase and lowercase leters in the scheme name. All other URI components are case-sensitive.  Applications MAY register themselves as handler for the \"iscc:\" URI scheme if no other handler is already registered. If another handler is already registered an application MAY ask the user to change it on the first run of the application.",
            "title": "ISCC URI Scheme"
        },
        {
            "location": "/specification/#uri-syntax",
            "text": "<foo>  means placeholder,  [bar]  means optional.  iscc:<fq-iscc-code>[?stream=<name>]",
            "title": "URI Syntax"
        },
        {
            "location": "/specification/#uri-example",
            "text": "iscc:11TcMGvUSzqoM1CqVA3ykFawyh1R1sH4Bz8A1of1d2Ju4VjWt26S?stream=smart-license",
            "title": "URI Example"
        },
        {
            "location": "/specification/#procedures-algorithms",
            "text": "",
            "title": "Procedures &amp; Algorithms"
        },
        {
            "location": "/specification/#base58-iscc",
            "text": "The  ISCC  uses a custom per-component data encoding similar to the  zbase62  encoding by  Zooko Wilcox-O'Hearn  but with a 58-symbol table. The encoding does not require padding and will always yield component codes of 13 characters length for ths 72-bit component digests. The predictable size of the encoding is a property that allows for easy composition and decomposition of components without having to rely on a delimiter (hyphen) in the  ISCC  code representation. Colliding body segments of the digest are preserved by encoding the header and body separately. The ASCII symbol table also minimizes transcription and OCR errors by omitting the easily confused characters  'O', '0', 'I', 'l' .",
            "title": "Base58-ISCC"
        },
        {
            "location": "/specification/#encode",
            "text": "Signature:  encode(digest: bytes) -> str  The  encode  function accepts a 9-byte  ISCC  Component Digest  and returns the Base58- ISCC  encoded  alphanumeric string of 13 characters which we call the  ISCC -Component Code .  See also:  Base- ISCC  Encoding reference code",
            "title": "encode"
        },
        {
            "location": "/specification/#decode",
            "text": "Signature: decode(code: str) -> bytes  the  decode  function accepts a 13- character   ISCC -Component Code  and returns the corresponding 9-byte  ISCC -Component Digest .  See also:  Base- ISCC  Decoding reference code",
            "title": "decode"
        },
        {
            "location": "/specification/#content-normalization",
            "text": "The  ISCC  standardizes some content normalization procedures to support reproducible and stable identifiers. Following the list of normalization functions that MUST be provided by a conforming implementation.",
            "title": "Content Normalization"
        },
        {
            "location": "/specification/#text_pre_normalize",
            "text": "Signature:  text_pre_normalize(text: str|bytes) -> str  Decodes raw plain-text data and applies Unicode  Normalization Form KC (NFKC)  . The plain-text data MUST be stripped of any markup beforhand. Text input is expected to be UTF-8 encoded plain-text data or a native type of the implementig programming language that supports Unicode. Text decoding errors MUST fail with an error.  See also:  Text pre-normalization reference code",
            "title": "text_pre_normalize"
        },
        {
            "location": "/specification/#text_trim",
            "text": "Signature:  text_trim(text: str) -> str  Trim text such that its UTF-8 encoded byte representation does not exceed 128-bytes each.  See also:  Text trimming reference code",
            "title": "text_trim"
        },
        {
            "location": "/specification/#text_normalize",
            "text": "Signature:  text_normalize(text: str) -> str  We define a text normalization function that is specific to our application. It takes unicode text as an input and returns  normalized  Unicode text for further algorithmic processing. The  text_normalize  function performs the following operations in the given order while each step works with the results of the previous operation:   Decompose the input text by applying  Unicode Normalization Form D (NFD) .  Filter and normalize text by iterating over unicode characters while:  replacing groups of one or more consecutive  Separator  characters ( Unicode categories  Zs, Zl and Zp) with exactly one Unicode  SPACE   character  ( U+0020 ) .  removing characters that are not in one of the Unicode categories  Separator  ,  Letter ,  Number  or  Symbol .  converting characters to lower case.  Remove any leading or trailing  Separator  characters.  Re-Compose the text by applying  Unicode Normalization Form C (NFC) .   See also:  Text normalization reference code",
            "title": "text_normalize"
        },
        {
            "location": "/specification/#image_normalize",
            "text": "Signature:  image_normalize(img) -> List[List[int]]  Accepts a file path, byte-stream or raw binary image data and MUST at least support JPEG, PNG, and GIF image formats. Normalize the image with the following steps:   Convertf the image to greyscale  Resize the image to 32x32 pixels using  bicubic interpolation  Create a 32x32 two-dimensional array of 8-bit grayscale values from the image data   See also:  Image normalization reference code",
            "title": "image_normalize"
        },
        {
            "location": "/specification/#feature-hashing",
            "text": "The  ISCC  standardizes various feature hashing algorithms that reduce content features to a binary vector used as the body of the various Content-ID components.",
            "title": "Feature Hashing"
        },
        {
            "location": "/specification/#similarity_hash",
            "text": "Signature:  similarity_hash(hash_digests: Sequenc[ByteString]) -> bytes  The  similarity_hash  function takes a sequence of hash digests which represent a set of features. Each of the digests MUST be of equal size. The function returns a new hash digest (raw 8-bit bytes) of the same size. For each bit in the input hashes calulate the number of hashes with that bit set and substract the the count of hashes where it is not set. For the output hash set the same bit position to  0  if the count is negative or  1  if it is zero or positive. The resulting hash digest will retain similarity for similar sets of input hashes. See also   [Charikar2002] .   See also:  Similarity hash reference code",
            "title": "similarity_hash"
        },
        {
            "location": "/specification/#minimum_hash",
            "text": "Signature:  minimum_hash(features: Iterable[int]) -> List[int]  The  minimum_hash  function takes an arbitrary sized set of 32-bit integer features and reduces it to a fixed size vector of 128 features such that it preserves similarity with other sets. It is based on the MinHash implementation of the  datasketch  library by  Eric Zhu .  See also:  Minimum hash reference code",
            "title": "minimum_hash"
        },
        {
            "location": "/specification/#image_hash",
            "text": "Signature:  image_hash(pixels: List[List[int]]) -> bytes   Perform a discrete cosine transform per row of input pixels.  Perform a discrete cosine transform per column on the resulting matrix from step 2.  Extract upper left 8x8 corner of array from step 2 as a flat list.  Calculate the median of the results from step 3.  Create a 64-bit digest by iterating over the values of step 5 and setting a   1 - for values above median and  0  for values below or equal to median.  Return results from step 5.   See also:  Image hash reference code",
            "title": "image_hash"
        },
        {
            "location": "/specification/#content-defined-chunking",
            "text": "For shift resistant data chunking the  ISCC  requires a custom chunking algorithm:",
            "title": "Content Defined Chunking"
        },
        {
            "location": "/specification/#data_chunks",
            "text": "Signature:  data_chunks(data: stream) -> Iterator[bytes]  The  data_chunks  function accepts a byte-stream and returns variable sized chunks. Chunk boundaries are determined by a gear based chunking algorithm based on  [WenXia2016] .  See also:  CDC  reference code",
            "title": "data_chunks"
        },
        {
            "location": "/specification/#conformance-testing",
            "text": "An application that claims  ISCC  conformance MUST pass the  ISCC  conformance test suite. The test suite is available as json data in our  Github Repository . Testdata is stuctured as follows:  { \n     \"<function_name>\" :   { \n         \"<test_name>\" :   { \n             \"inputs\" :   [ \"<value1>\" ,   \"<value2>\" ], \n             \"outputs\" :   [ \"value1>\" ,   \"<value2>\" ] \n         } \n     }  }",
            "title": "Conformance Testing"
        },
        {
            "location": "/license/",
            "text": "License\n#\n\n\nCC BY-NC-SA 4.0 License\n\n\nCopyright \u00a9 2016 - 2017 Content Blockchain Project\n\n\nThis work is licensed under a \nCreative Commons Attribution-ShareAlike 4.0 International License\n.",
            "title": "License"
        },
        {
            "location": "/license/#license",
            "text": "CC BY-NC-SA 4.0 License  Copyright \u00a9 2016 - 2017 Content Blockchain Project  This work is licensed under a  Creative Commons Attribution-ShareAlike 4.0 International License .",
            "title": "License"
        }
    ]
}