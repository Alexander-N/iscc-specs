{
    "docs": [
        {
            "location": "/",
            "text": "ISCC - International Standard Content Code\n#\n\n\n\n\nAttention\n\n\nThis site is work in progress!\n\n\n\n\nThe latest version of these pages can be found at \niscc.codes\n\n\nBetter Content Identifiers\n#\n\n\nCurrently, the media industry is mostly relying on identifiers that were designed for physical products such as printed books and magazines. However, traditional content identifiers (like ISBN, ISSN or ISRC) are managed centrally and fall short of the requirements for digital trade.\n\n\nFreely accessible standard \nidentifiers\n, which are specifically designed for \ndigital content\n and \nblockchain\n based management, are a fundamental prerequisite for transactions and sales activities in a digital and increasingly heterogeneous media environment.\n\n\nWith better identifiers for digital content, the entire ecosystem becomes more efficient.\n\n\n\n\nHow it works\n#\n\n\nThe major innovation of the \nISCC\n: identifiers are generated algorithmically and in a decentralized way from a basic set of metadata and the content itself. This inseparably links any specific content to a specific ID.\n\n\nThe \nISCC\n is a unique, hierarchically structured composite identifier. It is built from a generic and balanced mix of content-derived, locality-sensitive and similarity-preserving hashes generated from metadata and content.",
            "title": "Overview"
        },
        {
            "location": "/#iscc-international-standard-content-code",
            "text": "Attention  This site is work in progress!   The latest version of these pages can be found at  iscc.codes",
            "title": "ISCC - International Standard Content Code"
        },
        {
            "location": "/#better-content-identifiers",
            "text": "Currently, the media industry is mostly relying on identifiers that were designed for physical products such as printed books and magazines. However, traditional content identifiers (like ISBN, ISSN or ISRC) are managed centrally and fall short of the requirements for digital trade.  Freely accessible standard  identifiers , which are specifically designed for  digital content  and  blockchain  based management, are a fundamental prerequisite for transactions and sales activities in a digital and increasingly heterogeneous media environment.  With better identifiers for digital content, the entire ecosystem becomes more efficient.",
            "title": "Better Content Identifiers"
        },
        {
            "location": "/#how-it-works",
            "text": "The major innovation of the  ISCC : identifiers are generated algorithmically and in a decentralized way from a basic set of metadata and the content itself. This inseparably links any specific content to a specific ID.  The  ISCC  is a unique, hierarchically structured composite identifier. It is built from a generic and balanced mix of content-derived, locality-sensitive and similarity-preserving hashes generated from metadata and content.",
            "title": "How it works"
        },
        {
            "location": "/concept/",
            "text": "ISCC - Concept\n#\n\n\nThe internet is shifting towards a network of decentralized peer-to-peer transactions. If we want our transactions on the emerging blockchain networks to be about content we need standardized ways to address content. Our transactions might be payments, attributions, reputation, certification, licenses or entirely new kinds of value transfer. All this will happen much faster and easier if we, as a community, can agree on how to identify content in a decentralized environment. This is the first draft of an open proposal to the wider content community for a common content identifier. We would like to share our ideas and spark a conversation with journalists, news agencies, content creators, publishers, distributors, libraries, musicians, scientists, developers, lawyers, rights organizations and all the other participants of the content ecosystem.\n\n\nIntroduction\n#\n\n\nThere are many\n existing standards\n for media identifiers serving a wide array of use cases. For example book publishing uses the \nISBN\n, magazines have the \nISSN\n, music industry has \nISRC\n, film has \nISAN\n and science has \nDOI\n - each of them serving a set of specific purposes. These identifiers have important roles across many layers. The \nstructure and management\n of these \nglobal identifiers\n strongly correlates with the grade of achievable \nautomation\n and potential for \ninnovation\n within and across different sectors of the media industries. Some communities, like online journalism, don't even have any global persistent identifiers for their content.\n\n\nMany of the established standards manage registration of identifiers in \ncentralized \nor \nhierarchical systems\n involving manual and costly processes. Often the associated metadata is not easily or freely accessible for third parties (if available at all). The overhead, cost and general properties of these systems make them unsuitable for many innovative use cases. Existing and established standards have trouble keeping up with the fast evolving digital economy. For example nowadays major e-book retailers do not even require an \nISBN\n and instead establish their own proprietary identifiers. Amazon has the \nASIN\n, Apple has \nApple-ID\n and Google has \nGKEY\n. The fast paced development of the digital media economy has led to an increasing fragmentation of identifiers and new barriers in interoperability. For many tasks current systems need to track and match all the different vendor specific IDs, which is an inefficient and error prone process.\n\n\nAdvances in data structures, algorithms, machine learning and the uprise of crypto economics allows us to invent \nnew\n kinds of \nmedia identifiers\n and \nre-imagine existing identifiers\n with innovative use cases in mind. Blockchains and Smart Contracts offer great opportunities in solving many of the challenges of identifier registration, like centralized management, data duplication and disambiguation, vendor lock-in and long term data retention.\n\n\nThis is an open proposal to the digital media community and explores the possibilities of a \ndecentralized \ncontent identifier system. We\u2019d like to establish an open standard for persistent, unique, vendor independent and content derived cross-media identifiers that are stored and managed in a global and decentralized blockchain. We envision a self-governing ecosystem with a low barrier of entry where \ncommercial and non-commercial\n initiatives can both innovate and thrive next to each other.\n\n\nMedia Identifiers for Blockchains\n#\n\n\nMedia cataloging systems tend to get out of hand and become complex and often unmanageable. Our design proposal is focused on keeping the ISCC system as simple and more importantly as \nautomatable\n as possible, while maximizing practical value for the most important use cases \u2014 meaning you should get out more than you have to put in. With this in mind we come to the following basic design decisions:\n\n\nA \u201cMeaningful\u201d Identifier\n#\n\n\nIn traditional database systems it is recommended practice to work with \nsurrogate keys\n as identifiers. A surrogate key has no business meaning and is completely decoupled from the data it identifies. Uniqueness of such identifiers is guaranteed either via centralized incremental assignment by the database system or via random UUIDs which have a very low probability of collisions. While random UUIDs could be generated in a decentralized way, both approaches require some external authority that establishes or certifies the linkage between the identifier and the associated metadata and content. This is why we decided to go with a \u201cmeaningful\u201d \ncontent and metadata derived identifier (CMDI)\n. Anyone will be able to verify that a specific identifier indeed belongs to a given digital content. Even better, anyone can \u201cfind\u201d the identifier for a given content without the need to consult external data sources. This approach also captures essential information about the media in the identifier itself, which is very useful in scenarios of machine learning and data analytics.\n\n\nA Decentralized Identifier\n#\n\n\nWe would like our identifier to be registry agnostic. This means that identifiers can be self-issued in a decentralized and parallel fashion without the need to ask for permission. Even if identifiers are not registered in a central database or on a public blockchain they are still useful in cases where multiple independent parties exchange information about content. The CMDI approach is helpful with common issues like data integrity, validation, de-duplication and disambiguation.\n\n\nStorage Considerations\n#\n\n\nOn a typical public blockchain all data is \nfully replicated\n among participants. This allows for independent and autonomous validation of transactions. All blockchain data is highly available, tamper-proof and accessible for free. However, under high load the limited transaction capacity (storage space per unit of time) creates a transaction fee market. This leads to\n growing transaction costs\n and makes storage space a scarce and increasingly precious resource. So it is mandatory for our identifier and its eventual metadata schema to be very \nspace efficient \nto maximize benefit at minimum cost. The basic metadata that will be required to generate and register identifiers must be:\n\n\n\n\nminimal in scope\n\n\nclearly specified\n\n\nrobust against human error\n\n\nenforced on technical level\n\n\nadequate for public use (no legal or privacy issues)\n\n\n\n\nLayers of Digital Media Identification\n#\n\n\nWhile we examined existing identifiers we discovered that there is often much confusion about the extent or coverage of what exactly is being identified by a given system. With our idea for a generic cross-media identifier we want to put special weight on being precise with our definitions and found it helpful to distinguish between \u201cdifferent layers of digital media identification\". We found that these layers exist naturally on a scale from abstract to concrete. Our analysis also showed that existing standard identifiers only operate on one or at most two of such layers. The ISCC will be designed as a \ncomposite identifier\n that takes the different layers of media identification into consideration:\n\n\nLayer 1 \u2013 Abstract Creation\n#\n\n\nIn the first and most abstract layer we are concerned with distinguishing between different works or creations in the \nbroadest possible sense\n. The scope of identification is completely independent of any manifestations of the work, be it physical or digital in nature. It is also agnostic to creators, rights holders or any specific interpretations, expressions or language versions of a work. It only relates to the intangible creation - the idea itself.\n\n\nLayer 2 \u2013 Semantic Field\n#\n\n\nThis layer relates to the meaning or essence of a work. It is an amorphous collection or combination of facts, concepts, categories, subjects, topics, themes, assumptions, observations, conclusions, beliefs and other intangible things that the content conveys. The scope of identification is a set of coordinates within a finite and multidimensional semantic space.\n\n\nLayer 3 \u2013 Generic Manifestation\n#\n\n\nIn this layer we are concerned with the literal structure of a media type specific and normalized manifestation. Namely the basic text, image, audio or video content independent of its semantic meaning or media file encoding and with a tolerance to variation. This \"tolerance to variation\" bundles a set of different versions with corrections, revisions, edits, updates, personalizations, different format encodings or data compressions of the same content under one grouping identifier. A generic manifestation is independent of a final digital media product and is specific to an expression, version or interpretation of a work.\n\n\nUnfortunately it is not obvious where generic manifestation of a work ends and another one starts. It depends on human interpretation and context. How much editing do we allow before we call it a \u201cdifferent\u201d manifestation and give it a different identifier. A practical but only partial solution to this problem is to create a algorithmically defined and testable spectrum of tolerance to variation per media type. This can provide a stable and repeatable process to distinguish between generic content manifestations. But it is important to understand that such a process is not expected to yield results that are always intuitive to human expectations as to where exactly boundaries should be.\n\n\nLayer 4 \u2013 Media Specific Manifestation\n#\n\n\nThis layer relates to a \nmanifestation with a specific encoding\n. It identifies a \ndata-file\n encoded and offered in a specific \nmedia format \nincluding a tolerance to variation to account for minor edits and updates within a format without creating a new identifier. For example one could distinguish between the PDF, DOCX or WEBSITE versions of the same content as generated from a single source publishing system. This layer does only distinguish between products or \"artifacts\" with a given packaging or encoding.\n\n\nLayer 5 \u2013 Exact Representation\n#\n\n\nIn this layer we identify a data-file by its exact binary representation without any interpretation of meaning and without any ambiguity. Even a minimal change in data that might not change the interpretation of content would create a different identifier. Like the first four layers, this layer does also \nnot \nexpress any information related to \ncontent location\n or \nownership\n.\n\n\nLayer 6 \u2013 Individual Copy\n#\n\n\nIn the physical world we would call a specific book (one that you can take out of your shelve) an \nindividual copy\n. This implies a notion of \nlocality \nand \nownership\n. In the digital world the semantics of an individual copy are very different. An individual copy might be distinguished by a license you own or by a personalized watermark applied by the retailer at time of sale or some digital annotations you have added to your digital media file. While there can only ever be \none exact\n individual copy of a \nphysical object\n, there always can be \nendless replicas\n of an \"individual copy\" of a \ndigital object\n. It is very important to keep this difference in mind. Ignoring this fact has caused countless misunderstandings and is the source of confusion throughout the media industry \u2013 especially in realm of copyright and license discussions.\n\n\nWe could try to define an \nindividual digital copy\n by its location and exact content on a specific physical storage medium (like a DVD, SSD ...). But this does not account for the fact that it is nearly impossible to stop someone from creating an exact replica of that data or at least a snapshot or recording of the presentation of that data on another storage location.\n\n\nAnd most importantly such a replica does not affect the original data and even less can make it magically disappear. In contrast, if you give your individual copy of your book to someone else, you won't \n\"have it\"\n anymore. It is clear, that with digital media this \ncannot reliably be the case\n. The only way would be to build a \ntamper-proof physical device\n (secure element) that does not reveal the data itself, which would defeat the purpose by making the content itself unavailable. But there are ways to partially simulate such inherently physical properties in the digital world. Most notably with the emergence of blockchain technology it is now possible to have a \ncryptographically secured\n and publicly notarized tamper-proof \ncertificate of ownership. \n This can serve as a record of agreement about ownership of an \u201cindividual copy\u201d. But is does not by itself enforce location or accessibility of the content, nor does it prove the authorization of the certifying party itself or the legal validity of the agreement.\n\n\nAlgorithmic Tools\n#\n\n\nWhile many details about the ISCC are still up for discussion we are quite confident about some of the general algorithmic families that will make it into the final specification for the identifier. These will play an important role in how we generate the different components of the identifier:\n\n\n\n\nSimilarity preserving hash functions (Simhash, Minhash ...)\n\n\nPerceptual hashing (pHash, Blockhash, Chromaprint \u2026)\n\n\nContent defined chunking (Rabin-Karp, FastCDC ...)\n\n\nMerkle trees\n\n\n\n\nISCC Proof-of-Concept\n#\n\n\nBefore we settle on the details of the proposed ISCC identifier, we want to build a simple and reduced proof-of-concept implementation of our ideas. It will enable us and other developers to test with real world data and systems and find out early what works and what doesn't.\n\n\n\n\nThe minimal viable, first iteration ISCC will be a byte structure built from the following components:\n\n\nMeta-ID\n#\n\n\nThe MetaID will be generated as a similarity preserving hash from minimal generic metadata like \ntitle \nand \ncreators\n. It operates on \nLayer 1 \n and identifies an intangible creation. It is the first and most generic grouping element of the identifier. We will be experimenting with different n-gram sizes and bit-length to find the practical limits of precision and recall for generic metadata. We will also specify a process to disambiguate unintended collisions by adding optional metadata.\n\n\nPartial Content Flag\n#\n\n\nThe Partial Content Flag is a 1-bit flag that indicates if the remaining elements relate to the complete work or only to a subset of it.\n\n\nMedia Type Flag\n#\n\n\nThe Media Type Flag is a 3 bit flag that allows us to distinguish between up to 8 generic media types\n (GMTs)\n to which our ContentID component applies. We define a generic media type as\nbasic content type\n such as plain text or raw pixel data that will be specified exactly and extracted from more complex file formats or encodings. We will start with generic text and image types and add audio, video and mixed types later.\n\n\nContent-ID\n#\n\n\nThe ContentID operates on \nLayer 3\n and will be a GMT-specific similarity preserving hash generated from extracted content. It identifies the normalized content of a specific GMT, independent of file format or encoding. It relates to the structural essence of the content and groups similar GMT-specific manifestations of the abstract creation or parts of it (as indicated by the Partial Content Flag). For practical reasons we intentionally skip a \nLayer 2\n component at this time. It would add unnecessary complexity for a basic proof-of-concept implementation.\n\n\nData-ID\n#\n\n\nThe DataID operates on \nLayer 4 \nand will be a similarity preserving hash generated from shift-resistant content-defined chunks from the raw data of the encoded media blob. It groups complete encoded files with similar content and encoding. This component does not distinguish between GMTs as the files may include multiple different generic media types.\n\n\nInstance-ID\n#\n\n\nThe InstanceID operates on \nLayer 5 \nand will be the top hash of a merkle tree generated from (potentially content-defined) chunks of raw data of an encoded media blob. It identifies a concrete manifestation and proves the integrity of the full content. We use the merkle tree structure because it also allows as to verify integrity of partial chunks without having to have the full data available. This will be very useful in any scenarios of distributed data storage.\n\n\nWe intentionally skip \nLayer 6\n at this stage as content ownership and location will be handled on the blockchain layer of the stack and not by the ISCC identifier itself.\n\n\nA first experimental prototype of the ISCC idea is in development on \nGithub\n.",
            "title": "Concept"
        },
        {
            "location": "/concept/#iscc-concept",
            "text": "The internet is shifting towards a network of decentralized peer-to-peer transactions. If we want our transactions on the emerging blockchain networks to be about content we need standardized ways to address content. Our transactions might be payments, attributions, reputation, certification, licenses or entirely new kinds of value transfer. All this will happen much faster and easier if we, as a community, can agree on how to identify content in a decentralized environment. This is the first draft of an open proposal to the wider content community for a common content identifier. We would like to share our ideas and spark a conversation with journalists, news agencies, content creators, publishers, distributors, libraries, musicians, scientists, developers, lawyers, rights organizations and all the other participants of the content ecosystem.",
            "title": "ISCC - Concept"
        },
        {
            "location": "/concept/#introduction",
            "text": "There are many  existing standards  for media identifiers serving a wide array of use cases. For example book publishing uses the  ISBN , magazines have the  ISSN , music industry has  ISRC , film has  ISAN  and science has  DOI  - each of them serving a set of specific purposes. These identifiers have important roles across many layers. The  structure and management  of these  global identifiers  strongly correlates with the grade of achievable  automation  and potential for  innovation  within and across different sectors of the media industries. Some communities, like online journalism, don't even have any global persistent identifiers for their content.  Many of the established standards manage registration of identifiers in  centralized  or  hierarchical systems  involving manual and costly processes. Often the associated metadata is not easily or freely accessible for third parties (if available at all). The overhead, cost and general properties of these systems make them unsuitable for many innovative use cases. Existing and established standards have trouble keeping up with the fast evolving digital economy. For example nowadays major e-book retailers do not even require an  ISBN  and instead establish their own proprietary identifiers. Amazon has the  ASIN , Apple has  Apple-ID  and Google has  GKEY . The fast paced development of the digital media economy has led to an increasing fragmentation of identifiers and new barriers in interoperability. For many tasks current systems need to track and match all the different vendor specific IDs, which is an inefficient and error prone process.  Advances in data structures, algorithms, machine learning and the uprise of crypto economics allows us to invent  new  kinds of  media identifiers  and  re-imagine existing identifiers  with innovative use cases in mind. Blockchains and Smart Contracts offer great opportunities in solving many of the challenges of identifier registration, like centralized management, data duplication and disambiguation, vendor lock-in and long term data retention.  This is an open proposal to the digital media community and explores the possibilities of a  decentralized  content identifier system. We\u2019d like to establish an open standard for persistent, unique, vendor independent and content derived cross-media identifiers that are stored and managed in a global and decentralized blockchain. We envision a self-governing ecosystem with a low barrier of entry where  commercial and non-commercial  initiatives can both innovate and thrive next to each other.",
            "title": "Introduction"
        },
        {
            "location": "/concept/#media-identifiers-for-blockchains",
            "text": "Media cataloging systems tend to get out of hand and become complex and often unmanageable. Our design proposal is focused on keeping the ISCC system as simple and more importantly as  automatable  as possible, while maximizing practical value for the most important use cases \u2014 meaning you should get out more than you have to put in. With this in mind we come to the following basic design decisions:",
            "title": "Media Identifiers for Blockchains"
        },
        {
            "location": "/concept/#a-meaningful-identifier",
            "text": "In traditional database systems it is recommended practice to work with  surrogate keys  as identifiers. A surrogate key has no business meaning and is completely decoupled from the data it identifies. Uniqueness of such identifiers is guaranteed either via centralized incremental assignment by the database system or via random UUIDs which have a very low probability of collisions. While random UUIDs could be generated in a decentralized way, both approaches require some external authority that establishes or certifies the linkage between the identifier and the associated metadata and content. This is why we decided to go with a \u201cmeaningful\u201d  content and metadata derived identifier (CMDI) . Anyone will be able to verify that a specific identifier indeed belongs to a given digital content. Even better, anyone can \u201cfind\u201d the identifier for a given content without the need to consult external data sources. This approach also captures essential information about the media in the identifier itself, which is very useful in scenarios of machine learning and data analytics.",
            "title": "A \u201cMeaningful\u201d Identifier"
        },
        {
            "location": "/concept/#a-decentralized-identifier",
            "text": "We would like our identifier to be registry agnostic. This means that identifiers can be self-issued in a decentralized and parallel fashion without the need to ask for permission. Even if identifiers are not registered in a central database or on a public blockchain they are still useful in cases where multiple independent parties exchange information about content. The CMDI approach is helpful with common issues like data integrity, validation, de-duplication and disambiguation.",
            "title": "A Decentralized Identifier"
        },
        {
            "location": "/concept/#storage-considerations",
            "text": "On a typical public blockchain all data is  fully replicated  among participants. This allows for independent and autonomous validation of transactions. All blockchain data is highly available, tamper-proof and accessible for free. However, under high load the limited transaction capacity (storage space per unit of time) creates a transaction fee market. This leads to  growing transaction costs  and makes storage space a scarce and increasingly precious resource. So it is mandatory for our identifier and its eventual metadata schema to be very  space efficient  to maximize benefit at minimum cost. The basic metadata that will be required to generate and register identifiers must be:   minimal in scope  clearly specified  robust against human error  enforced on technical level  adequate for public use (no legal or privacy issues)",
            "title": "Storage Considerations"
        },
        {
            "location": "/concept/#layers-of-digital-media-identification",
            "text": "While we examined existing identifiers we discovered that there is often much confusion about the extent or coverage of what exactly is being identified by a given system. With our idea for a generic cross-media identifier we want to put special weight on being precise with our definitions and found it helpful to distinguish between \u201cdifferent layers of digital media identification\". We found that these layers exist naturally on a scale from abstract to concrete. Our analysis also showed that existing standard identifiers only operate on one or at most two of such layers. The ISCC will be designed as a  composite identifier  that takes the different layers of media identification into consideration:",
            "title": "Layers of Digital Media Identification"
        },
        {
            "location": "/concept/#layer-1-abstract-creation",
            "text": "In the first and most abstract layer we are concerned with distinguishing between different works or creations in the  broadest possible sense . The scope of identification is completely independent of any manifestations of the work, be it physical or digital in nature. It is also agnostic to creators, rights holders or any specific interpretations, expressions or language versions of a work. It only relates to the intangible creation - the idea itself.",
            "title": "Layer 1 \u2013 Abstract Creation"
        },
        {
            "location": "/concept/#layer-2-semantic-field",
            "text": "This layer relates to the meaning or essence of a work. It is an amorphous collection or combination of facts, concepts, categories, subjects, topics, themes, assumptions, observations, conclusions, beliefs and other intangible things that the content conveys. The scope of identification is a set of coordinates within a finite and multidimensional semantic space.",
            "title": "Layer 2 \u2013 Semantic Field"
        },
        {
            "location": "/concept/#layer-3-generic-manifestation",
            "text": "In this layer we are concerned with the literal structure of a media type specific and normalized manifestation. Namely the basic text, image, audio or video content independent of its semantic meaning or media file encoding and with a tolerance to variation. This \"tolerance to variation\" bundles a set of different versions with corrections, revisions, edits, updates, personalizations, different format encodings or data compressions of the same content under one grouping identifier. A generic manifestation is independent of a final digital media product and is specific to an expression, version or interpretation of a work.  Unfortunately it is not obvious where generic manifestation of a work ends and another one starts. It depends on human interpretation and context. How much editing do we allow before we call it a \u201cdifferent\u201d manifestation and give it a different identifier. A practical but only partial solution to this problem is to create a algorithmically defined and testable spectrum of tolerance to variation per media type. This can provide a stable and repeatable process to distinguish between generic content manifestations. But it is important to understand that such a process is not expected to yield results that are always intuitive to human expectations as to where exactly boundaries should be.",
            "title": "Layer 3 \u2013 Generic Manifestation"
        },
        {
            "location": "/concept/#layer-4-media-specific-manifestation",
            "text": "This layer relates to a  manifestation with a specific encoding . It identifies a  data-file  encoded and offered in a specific  media format  including a tolerance to variation to account for minor edits and updates within a format without creating a new identifier. For example one could distinguish between the PDF, DOCX or WEBSITE versions of the same content as generated from a single source publishing system. This layer does only distinguish between products or \"artifacts\" with a given packaging or encoding.",
            "title": "Layer 4 \u2013 Media Specific Manifestation"
        },
        {
            "location": "/concept/#layer-5-exact-representation",
            "text": "In this layer we identify a data-file by its exact binary representation without any interpretation of meaning and without any ambiguity. Even a minimal change in data that might not change the interpretation of content would create a different identifier. Like the first four layers, this layer does also  not  express any information related to  content location  or  ownership .",
            "title": "Layer 5 \u2013 Exact Representation"
        },
        {
            "location": "/concept/#layer-6-individual-copy",
            "text": "In the physical world we would call a specific book (one that you can take out of your shelve) an  individual copy . This implies a notion of  locality  and  ownership . In the digital world the semantics of an individual copy are very different. An individual copy might be distinguished by a license you own or by a personalized watermark applied by the retailer at time of sale or some digital annotations you have added to your digital media file. While there can only ever be  one exact  individual copy of a  physical object , there always can be  endless replicas  of an \"individual copy\" of a  digital object . It is very important to keep this difference in mind. Ignoring this fact has caused countless misunderstandings and is the source of confusion throughout the media industry \u2013 especially in realm of copyright and license discussions.  We could try to define an  individual digital copy  by its location and exact content on a specific physical storage medium (like a DVD, SSD ...). But this does not account for the fact that it is nearly impossible to stop someone from creating an exact replica of that data or at least a snapshot or recording of the presentation of that data on another storage location.  And most importantly such a replica does not affect the original data and even less can make it magically disappear. In contrast, if you give your individual copy of your book to someone else, you won't  \"have it\"  anymore. It is clear, that with digital media this  cannot reliably be the case . The only way would be to build a  tamper-proof physical device  (secure element) that does not reveal the data itself, which would defeat the purpose by making the content itself unavailable. But there are ways to partially simulate such inherently physical properties in the digital world. Most notably with the emergence of blockchain technology it is now possible to have a  cryptographically secured  and publicly notarized tamper-proof  certificate of ownership.   This can serve as a record of agreement about ownership of an \u201cindividual copy\u201d. But is does not by itself enforce location or accessibility of the content, nor does it prove the authorization of the certifying party itself or the legal validity of the agreement.",
            "title": "Layer 6 \u2013 Individual Copy"
        },
        {
            "location": "/concept/#algorithmic-tools",
            "text": "While many details about the ISCC are still up for discussion we are quite confident about some of the general algorithmic families that will make it into the final specification for the identifier. These will play an important role in how we generate the different components of the identifier:   Similarity preserving hash functions (Simhash, Minhash ...)  Perceptual hashing (pHash, Blockhash, Chromaprint \u2026)  Content defined chunking (Rabin-Karp, FastCDC ...)  Merkle trees",
            "title": "Algorithmic Tools"
        },
        {
            "location": "/concept/#iscc-proof-of-concept",
            "text": "Before we settle on the details of the proposed ISCC identifier, we want to build a simple and reduced proof-of-concept implementation of our ideas. It will enable us and other developers to test with real world data and systems and find out early what works and what doesn't.   The minimal viable, first iteration ISCC will be a byte structure built from the following components:",
            "title": "ISCC Proof-of-Concept"
        },
        {
            "location": "/concept/#meta-id",
            "text": "The MetaID will be generated as a similarity preserving hash from minimal generic metadata like  title  and  creators . It operates on  Layer 1   and identifies an intangible creation. It is the first and most generic grouping element of the identifier. We will be experimenting with different n-gram sizes and bit-length to find the practical limits of precision and recall for generic metadata. We will also specify a process to disambiguate unintended collisions by adding optional metadata.",
            "title": "Meta-ID"
        },
        {
            "location": "/concept/#partial-content-flag",
            "text": "The Partial Content Flag is a 1-bit flag that indicates if the remaining elements relate to the complete work or only to a subset of it.",
            "title": "Partial Content Flag"
        },
        {
            "location": "/concept/#media-type-flag",
            "text": "The Media Type Flag is a 3 bit flag that allows us to distinguish between up to 8 generic media types  (GMTs)  to which our ContentID component applies. We define a generic media type as basic content type  such as plain text or raw pixel data that will be specified exactly and extracted from more complex file formats or encodings. We will start with generic text and image types and add audio, video and mixed types later.",
            "title": "Media Type Flag"
        },
        {
            "location": "/concept/#content-id",
            "text": "The ContentID operates on  Layer 3  and will be a GMT-specific similarity preserving hash generated from extracted content. It identifies the normalized content of a specific GMT, independent of file format or encoding. It relates to the structural essence of the content and groups similar GMT-specific manifestations of the abstract creation or parts of it (as indicated by the Partial Content Flag). For practical reasons we intentionally skip a  Layer 2  component at this time. It would add unnecessary complexity for a basic proof-of-concept implementation.",
            "title": "Content-ID"
        },
        {
            "location": "/concept/#data-id",
            "text": "The DataID operates on  Layer 4  and will be a similarity preserving hash generated from shift-resistant content-defined chunks from the raw data of the encoded media blob. It groups complete encoded files with similar content and encoding. This component does not distinguish between GMTs as the files may include multiple different generic media types.",
            "title": "Data-ID"
        },
        {
            "location": "/concept/#instance-id",
            "text": "The InstanceID operates on  Layer 5  and will be the top hash of a merkle tree generated from (potentially content-defined) chunks of raw data of an encoded media blob. It identifies a concrete manifestation and proves the integrity of the full content. We use the merkle tree structure because it also allows as to verify integrity of partial chunks without having to have the full data available. This will be very useful in any scenarios of distributed data storage.  We intentionally skip  Layer 6  at this stage as content ownership and location will be handled on the blockchain layer of the stack and not by the ISCC identifier itself.  A first experimental prototype of the ISCC idea is in development on  Github .",
            "title": "Instance-ID"
        },
        {
            "location": "/specification/",
            "text": "ISCC\n - Specification v0.9.1\n#\n\n\n\n\nAttention\n\n\nThis document is a work in progress draft! It may be updated, replaced, or obsoleted by other documents at any time. This document must not be used as reference material or cited other than as \"work in progress\".\n\n\n\n\nAbstract\n#\n\n\nThe \nInternational Standard Content Code (\nISCC\n)\n, is an open and decentralized digital media identifier. An \nISCC\n can be created from digital content and its basic metadata by anybody who follows the procedures of the \nISCC\n specification or by using open source software that supports \nISCC\n creation \nconforming to the \nISCC\n specification\n.\n\n\nNote to Readers\n#\n\n\nFor public discussion of issues for this draft please use the Github issue tracker: \nhttps://github.com/coblo/iscc-specs/issues\n.\n\n\nThe latest published version of this draft can be found at \nhttp://iscc.codes/specification/\n. \n\n\nPublic review, discussion and contributions are welcome.\n\n\nAbout this Document\n#\n\n\nThis document proposes an open and vendor neutral \nISCC\n standard and describes the technical procedures to create and manage \nISCC\n identifiers. The first version of this document is produced by the \nContent Blockchain Project\n as a prototype and received funding from the \nGoogle Digital News Initiative (DNI)\n. The content of this document is determined by its authors in an open and public consensus process.\n\n\nConventions and Terminology\n#\n\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in \nRFC 2119\n [RFC2119].\n\n\nDefinitions\n#\n\n\n\n\nBasic Metadata:\n\n\nMinimal set of required metadata about the digital media object that is identified by an \nISCC\n.\n\n\nCharacter:\n\n\nThroughout this specification a \ncharacter\n is meant to be interpreted as one Unicode code point. This also means that due to the structure of Unicode a \ncharacter\n is not necessarily a full glyph but might be a combining accent or similar.\n\n\nDigital Media Object:\n\n\nA blob of raw bytes with some media type specific encoding.\n\n\nExtended Metadata:\n\n\nMetadata that is not encoded within the \nISCC\n but may be supplied together with the \nISCC\n.\n\n\nGeneric Media Type:\n\n\nA basic content type such as plain text in a normalized and \ngeneric\n (\nUTF-8\n) encoding format.\n\n\nISCC\n:\n\n\nInternational Standard Content Code\n\n\nISCC Code\n:\n\n\nThe printable text encoded representation of an \nISCC\n\n\nISCC Digest\n:\n\n\nThe raw binary data of an \nISCC\n\n\n\n\nIntroduction\n#\n\n\nAn \nISCC\n permanently identifies the content of a given digital media object at multiple levels of \ngranularity\n. It is algorithmically generated from basic metadata and the contents of the digital media object which it identifies. It is designed for being registered and stored on a public and decentralized blockchain. An \nISCC\n for a media object can be created and registered by the content author, a publisher, a service provider or anybody else. By itself the \nISCC\n and its basic registration on a blockchain does not make any statement or claim about authorship or ownership of the identified content.\n\n\nISCC\n Structure\n#\n\n\nThe \nISCC Digest\n is a fixed size sequence of 36 bytes (288 bits) assembled from multiple sub-components. The printable \nISCC Code\n is a 52 \ncharacter\n encoded string representation of an \nISCC Digest\n. This is a high-level overview of the \nISCC\n creation process:\n\n\n\n\nISCC\n Components\n#\n\n\nThe \nISCC Digest\n is built from multiple self-describing 72-bit components:\n\n\n\n\n\n\n\n\nComponents:\n\n\nMeta-ID\n\n\nContent-ID\n\n\nData-ID\n\n\nInstance-ID\n\n\n\n\n\n\n\n\n\n\nContext:\n\n\nIntangible creation\n\n\nContent similarity\n\n\nData similarity\n\n\nData checksum\n\n\n\n\n\n\nInput:\n\n\nMetadata\n\n\nExtracted  content\n\n\nRaw data\n\n\nRaw data\n\n\n\n\n\n\nAlgorithms:\n\n\nSimilarity Hash\n\n\nType specific\n\n\nCDC\n, Minimum Hash\n\n\nCDC\n, Hash Tree\n\n\n\n\n\n\nSize:\n\n\n72 bits\n\n\n72 bits\n\n\n72 bits\n\n\n72 bits\n\n\n\n\n\n\n\n\nThese components may be used independently by applications for various purposes but must be combined into a 52 \ncharacter\n \nbase58-iscc\n encoded string (55 with hyphens) for a fully qualified \nISCC Code\n. The components must be combined in the fixed order of Meta-ID, Content-ID, Data-ID, Instance-ID and may be separated by hyphens.\n\n\n\n\nExamples\n\n\nPrintable output:\n\n\nISCC\n: 11cS7Y9NjD6DX-1DVcUdv5ewjDQ-1Qhwz8x54CShu-1d8uCbWCNbGWg\n\n\nMachine readable URI:\n\n\niscc:11TcMGvUSzqoM1CqVA3ykFawyh1R1sH4Bz8A1of1d2Ju4VjWt26S\n\n\n\n\nComponent Types\n#\n\n\nEach component has the same basic structure of a \n1-byte header\n and a \n8-byte body\n section. \n\n\nThe 1-byte header of each component is subdivided into 2 nibbles (4 bits). The first nibble specifies the component type while the second nibble is component specific.\n\n\nThe header only needs to be carried in the encoded representation. As similarity searches accross different components are of little use, the type information contained in the header of each component can be safely ignored after an \nISCC\n has been decomposed and internaly typed by an application. \n\n\nThe body section of each component is always 8-bytes and can thus be fit into a 64-bit integer for efficient data processing. \n\n\n\n\n\n\n\n\nComponent\n\n\nNibble-1\n\n\nNibble-2\n\n\nByte\n\n\n\n\n\n\n\n\n\n\nMeta-ID\n\n\n0000\n\n\n0000 - \nISCC\n version (0)\n\n\n0x00\n\n\n\n\n\n\nContent-ID\n-Text\n\n\n0001\n\n\n0000 - Content Type Text\n\n\n0x10\n\n\n\n\n\n\nContent-ID-Text \nPCF\n\n\n0001\n\n\n0001 - Content Type Text  + \nPCF\n\n\n0x11\n\n\n\n\n\n\nContent-ID-Image\n\n\n0001\n\n\n0010 - Content Type Image\n\n\n0x12\n\n\n\n\n\n\nContent-ID-Image \nPCF\n\n\n0001\n\n\n0011 - Content Type Image + \nPCF\n\n\n0x13\n\n\n\n\n\n\nContent-ID-Audio\n\n\n0001\n\n\n0100 - Content Type Audio\n\n\n0x14\n\n\n\n\n\n\nContent-ID-Audio \nPCF\n\n\n0001\n\n\n0101 - Content Type Audio + \nPCF\n\n\n0x15\n\n\n\n\n\n\nData-ID\n\n\n0010\n\n\n0000 - Reserved\n\n\n0x20\n\n\n\n\n\n\nInstance-ID\n\n\n0011\n\n\n0000 - Reserved\n\n\n0x30\n\n\n\n\n\n\n\n\nMeta-ID Component\n#\n\n\nThe Meta-ID component starts with a 1-byte header \n00000000\n. The first nibble \n0000\n indicates that this is a Meta-ID component type. The second nibble \n0000\n indicates that it belongs to an \nISCC\n of version 1. All subsequent components are expected to follow the specification of a version 1 \nISCC\n.\n\n\nThe Meta-ID body is built from a 64-bit \nsimilarity_hash\n over 4-\ncharacter\n n-grams of the basic metadata of the content to be identified.  The basic metadata supplied to the META-ID generating function is assumed to be UTF-8 encoded. Errors that occur during the decoding of such a bytestring input to a native Unicode must terminate the process and must not be silenced. An \nISCC\n generating application must provide a \nmeta_id\n function that accepts minimal and generic metadata and returns a \nBase58-\nISCC\n encoded\n Meta-ID component and trimmed metadata.\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nRequired\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntitle\n\n\ntext\n\n\nYes\n\n\nThe title of an intangible creation.\n\n\n\n\n\n\nextra\n\n\ntext\n\n\nNo\n\n\nA short statement that distinguishes this intangible creation from another one. (default: empty string)\n\n\n\n\n\n\nversion\n\n\ninteger\n\n\nNo\n\n\nISCC\n version number. (default: 0)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe basic metadata inputs are intentionally simple and generic. We abstain from more specific metadata for Meta-ID generation in favor of compatibility accross industries. Imagine a \ncreators\n input-field for metadata. Who would you list as the creators of a movie? The directors, writers the main actors? Would you list some of them or if not how do you decide whom you will list. All disambiguation of similar title data can be acomplished with the extra-field. Industry- and application-specific metadata requirements can be supplied as extended metadata with \nISCC\n registration.\n\n\n\n\nGenerate Meta-ID\n#\n\n\nAn \nISCC\n generating application must follow these steps in the given order to produce a stable Meta-ID:\n\n\n\n\nApply Unicode standard \nNormalization Form KC (NFKC)\n separately to the  \ntitle\n and \nextra\n inputs.\n\n\nTrim \ntitle\n and \nextra\n, such that their UTF-8 encoded byte representation does not exceed 128-bytes each. \nThe results of this step must be supplied as basic metadata for \nISCC\n registration.\n\n\nConcatenate trimmed\ntitle\n and \nextra\n from using a space ( \n\\u0020\n) as a seperator.\n\n\nApply \nnormalize_text\n to the results of step 3.\n\n\nCreate a list of 4 \ncharacter\n \nn-grams\n by sliding \ncharacter\n-wise through the result of step 4.\n\n\nEncode each n-gram from step 5 to an UTF-8 bytestring and calculate its \nxxHash64\n digest.\n\n\nApply \nsimilarity_hash\n to the list of digests from step 6.\n\n\nPrepend the 1-byte component header according to component type and \nISCC\n version (e.g. \n0x00\n) to the results of step 7.\n\n\nEncode the resulting 9 byte sequence with \nBase58-\nISCC\n Encoding\n\n\nReturn encoded Meta-ID, trimmed \ntitle\n and trimmed \nextra\n data.\n\n\n\n\n\n\nText trimming\n\n\nWhen trimming text be sure to trim the byte-length of the UTF-8 encoded version and not the number of characters. The trim point must be such, that it does not cut into multibyte characters. Characters might have different UTF-8 byte-length. For example \n\u00fc\n is 2-bytes, \n\u9a69\n is 3-bytes and \n\ud841\udf0e\n is 4-bytes. So the trimmed version of a string with 128 \n\u9a69\n-characters will result in a 42-\ncharacter\n string with a 126-byte UTF-8 encoded length. This is necessary because the results of this operation will be stored as basic metadata with strict byte size limits on the blockchain. \n\n\n\n\n\n\nPre-normalization\n\n\nApplications that perform automated data-ingestion should apply a custimized preliminary normalization to title data tailored to the dataset. Depending on catalog data removing pairs of brackets [], (), {}, and text inbetween them or cutting all text after the first occurence of a semicolon (;) or colon (:) can vastly improve de-duplication. \n\n\n\n\nDealing with Meta-ID collisions\n#\n\n\nIdeally we want multiple ISCCs that identify different manifestations of the \nsame intangible creation\n to be automatically grouped by an identical leading Meta-ID component. We call such a natural grouping an \nintended component collision\n. Metadata, captured and edited by humans, is notoriously unreliable. By using normalization and a similarity hash on the metadata we account for some of this variation while keeping the Meta-ID component somewhat stable. \n\n\nAuto-generated Meta-IDs components are \nexpected\n to miss some intended collisions. An application should check for such \nmissed intended component collisions\n before registering a new Meta-ID with the \ncanonical registry\n of ISCCs by conducting a similarity search and asking for user feedback.\n\n\nBut what about \nunintended component collisions\n? Such collisions might happen because two \ndifferent intangible creations\n have very similar or even identical metadata. But they might also happen simply by chance. With 2^56 possibile Meta-ID components the probability of random collisions rises in an S-cuved shape with the number of deployed ISCCs (see: \nHash Collision Probabilities\n).  We should keep in mind that, the Meta-ID component is only one part of a fully qualified \nISCC Code\n. Unintended collisions of the Meta-ID component are generally deemed as \nacceptable and expected\n. \n\n\nIf for any reason an application wants to avoid unintended collisions with pre-existing Meta-ID components it may utilze the \nextra\n-field. An application must first generate a Meta-ID without asking the user for input to the \nextra\n-field and then first check for collisions with the \ncanonical registry\n of ISCCs. After it finds a collision with a pre-existing Meta-ID it may display the metadata of the colliding entry and interact with the user to determine if it indeed is an unintended collision. Only if the user indicates an unintended collision, may the application ask for a disambiguation that is than added as an ammendment to the metadata via the \nextra\n-field to create a different Meta-ID component. The application may repeat the pre-existence check until it finds no collision or a user intended collision. The application must not supply autogenerated input to the \nextra\n-field.\n\n\nIt is our opinion that the concept of \nintended collisions\n of Meta-ID components is generally usefull concept and a net positive. But one must be aware that this characteristic also has its pitfalls. It is by no means an attempt to provide an unambigous - agreed upon - definition of \n\"identical intangible creations\"\n.\n\n\nContent-ID\n#\n\n\nThe Content-ID component has multiple subtypes. The subtypes correspond with the \nGeneric Media Types (\nGMT\n)\n. A fully qualified \nISCC\n can only have a Content-ID component of one specific \nGMT\n, but there may be multiple ISCCs with different Content-ID types per digital media object.\n\n\nA Content-ID is generated in two broad steps. In the first step, we extract and convert content from a rich media type to a normalized \nGMT\n. In the second step, we use a \nGMT\n-specific process to generate the Content-ID component of an \nISCC\n. \n\n\nGeneric Media Types\n#\n\n\nThe  Content-ID type is signaled by the first 3 bits of the second nibble of the first byte of the Content-ID:\n\n\n\n\n\n\n\n\nConent-ID Type\n\n\nNibble-2 Bits 0-3\n\n\n\n\n\n\n\n\n\n\ntext\n\n\n000\n\n\n\n\n\n\nimage\n\n\n001\n\n\n\n\n\n\naudio\n\n\n010\n\n\n\n\n\n\nvideo\n\n\n011\n\n\n\n\n\n\nmixed\n\n\n100\n\n\n\n\n\n\nReserved\n\n\n101, 110, 111\n\n\n\n\n\n\n\n\nContent-ID-Text\n#\n\n\nThe Content-ID-Text is built from the extracted plain-text content of an encoded media object. To build a stable Content-ID-Text the plain-text content must first be extracted from the digital media object. It should be extracted in a way that is reproducible. There are many different text document formats out in the wilde and extracting plain-text from all of them is anything but a trivial task. While text-extraction is out of scope for this specification it is recommend, that plain-text content should be extracted with the open-source \nApache Tika v1.17\n toolkit, if a generic reproducibility of the Content-ID-Text component is desired. \n\n\nAn \nISCC\n generating application must provide a \ncontent_id(text, partial=False)\n function that accepts UTF-8 encoded plain text and a boolean indicating the \npartial content flag\n as input and returns a Content-ID with \nGMT\n type \ntext\n. The procedure to create a Content-ID-Text is as follows:\n\n\n\n\nApply Unicode standard \nNormalization Form KC (NFKC)\n to the text input.\n\n\nApply \nnormalize_text\n to the text input.\n\n\nSplit the normalized text into a list of words at whitespace boundaries.\n\n\nCreate a list of 5 word shingles by sliding word-wise through the list of words.\n\n\nCreate  a list of 32-bit unsigned integer features by applying \nxxHash32\n to shingles from step 4.\n\n\nApply \nminimum_hash\n to the list of features from step 5.\n\n\nCollect the least significant bits from the 128 MinHash features from step 6.\n\n\nCreate two 64-bit digests from the first and second half of the collected bits.\n\n\nApply \nsimilarity_hash\n to the digests returned from step 8.\n\n\nPrepend the 1-byte component header (\n0x10\n full content or \n0x11\n partial content).\n\n\nEncode and return the resulting 9-byte sequence with \nBase58-\nISCC\n Encoding\n.\n\n\n\n\nContent-ID-Image\n#\n\n\nFor the Content-ID-Image we are opting for a DCT-based perceptual image hash instead of a more sophisticated keypoint detection based method. In view of the generic deployabiility of the \nISCC\n we chose an algorithm that has moderate computation requirements and is easy to implement while still being robust against most common minor image manipulations. \n\n\nAn \nISCC\n generating application must provide a \ncontent_id_image(image, partial=False)\n function that accepts a local file path to an image and returns a Content-ID with \nGMT\n type \nimage\n. The procedure to create a Content-ID-Image is as follows:\n\n\n\n\nConvert image to greyscale\n\n\nResize the image to 32x32 pixels using \nbicubic interpolation\n\n\nCreate a 32x32 two-dimensional array of 8-bit greyscale values from the image data\n\n\nPerform a discrete cosine transform per row\n\n\nPerform a DCT per column on the resulting matrix from step 4.\n\n\nExtract upper left 8x8 corner of array from step 4 as a flat list\n\n\nCalculate the median of the results from step 5\n\n\nCreate a 64-bit digest by iterating over the values of step 5 and setting a  \n1\n- for values above median and \n0\n for values below or equal to median.\n\n\nPrepend the 1-byte component header (\n0x12\n full content or \n0x13\n partial content)\n\n\nEncode and return the resulting 9-byte sequence with \nBase58-\nISCC\n Encoding\n\n\n\n\n\n\nImage Data Input\n\n\nThe \ncontent_id_image\n function may optionally accept the raw byte data of an encoded image or an internal native image object as input for convenience.\n\n\n\n\nPartial Content Flag (\nPCF\n)\n#\n\n\nThe last bit of the header byte is the \"Partial Content Flag\". It designates if the Content-ID applies to the full content or just some part of it. The \nPCF\n must be set as a \n0\n-bit (\nfull \nGMT\n-specific content\n) by default. Setting the \nPCF\n to \n1\n enables applications to create multiple ISCCs for partial extracts of one and the same digital file. The exact semantics of \npartial content\n are outside of the scope of this specification. Applications that plan to support partial Content-IDs should clearly define their semantics. For example, an application might create separate \nISCC\n for the text contents of multiple articles of a magazine issue. In such a scenario\nthe Meta-, Data-, and Instance-IDs are the compound key for the magazine issue, while the Content-ID-Text component distinguishes the different articles of the issue. The different Content-ID-Text components would automatically be \"bound\" together by the other 3 components.\n\n\nData-ID\n#\n\n\nFor the Data-ID that should encode data similarty we use content defined chunking algorithm that provides some shift resistance and calculate the MinHash from those chunks. To accomodate for small files the first 100 chunks have a ~140-byte size target while the remaining chunks target ~ 6kb in size.\n\n\nThe Data-ID is built from the raw encoded data of the content to be identified. An \nISCC\n generating application must provide a \ndata_id\n function that accepts the raw encoded data as input. Generate a Data-ID by this procedure:\n\n\n\n\nApply \nchunk_data\n to the raw encoded content data.\n\n\nFor each chunk calculate the xxHash32 integer hash.\n\n\nApply \nminimum_hash\n to the resulting list of 32-bit unsigned integers.\n\n\nCollect the least significant bits from the 128 MinHash features.\n\n\nCreate two 64-bit digests from the first and second half of the collected bits.\n\n\nApply \nsimilarity_hash\n to the results of step 5.\n\n\nPrepend the 1-byte component header (e.g. 0x20).\n\n\nEncode and return the resulting 9-byte sequence with \nBase58-\nISCC\n Encoding\n.\n\n\n\n\nInstance-ID\n#\n\n\nThe Instance-ID is built from the raw data of the media object to be identified and serves as checksum for the media object. The raw data of the media object is split into 64-kB data-chunks. Then we build a hash-tree from those chunks and use the truncated top-hash (merkle root) as component body of the Instance-ID.\n\n\nTo guard against length-extension attacks and second pre-image attacks we use double sha256 for hashing. We also prefix the hash input data with a \n0x00\n-byte for the leaf nodes hashes and with a \n0x01\n-byte for the  internal node hashes. While the Instance-ID itself is a non-cryptographic checksum, the full top-hash may be supplied in the extended metadata of an \nISCC\n secure integrity verification is required.\n\n\n\n\nAn \nISCC\n generating application must provide a \ninstance_id\n function that accepts the raw data file as input and returns an encoded Instance-ID and a full hex-encoded 256-bit top-hash. Generate an Instance-ID by this procedure:\n\n\n\n\nSplit the raw bytes of the encoded media object into 64-kB chunks.\n\n\nFor each chunk calculate the \nsha256d\n of the concatenation of a \n0x00\n-byte and the chunk bytes. We call the resulting values \nleaf node hashes\n (\nLNH\n).\n\n\nCalculate the next level of the hash tree by applying \nsha256d\n to the concatenation of a \n0x01\n-byte and adjacent pairs of \nLNH\n values. If the length of the list of \nLNH\n values is uneven concatenate the last \nLNH\n value with itself. We call the resulting values \ninternal node hashes\n (\nINH\n).\n\n\nRecursively apply \n0x01\n-prefixed pairwise hashing to the results of  step 3 until the process yields only one hash value. We call this value the top-hash.\n\n\nTrim the resulting \ntop hash\n to the first 8 bytes.\n\n\nPrepend the 1-byte component header (e.g. \n0x30\n).\n\n\nEncode resulting 9-byte sequence with \nBase58-\nISCC\n Encoding\n to an Instance-ID Code\n\n\nHex-Encode the \ntop hash\n \n\n\nReturn the Intance-ID and the hex-encoded top-hash\n\n\n\n\nApplications may carry, store, and process the leaf node hashes or even the full hash-tree for advanced streaming data identification or partial data integrity verification.\n\n\nProcedures & Algorithms\n#\n\n\nBase58-\nISCC\n Encoding\n#\n\n\nThe \nISCC\n uses a custom per-component data encoding that is based on the \nzbase62\n encoding by \nZooko Wilcox-O'Hearn\n. The encoding does not require padding and will always yield component codes of 13 characters length for our 72-bit digests. The predictable size of the encoding is a property that allows for easy composition and decomposition of components without having to rely on a delimiter (hyphen) in the \nISCC\n code representation. Colliding body segments of the digest are preserved by encoding the header and body separately. The symbol table also minimizes transcription and OCR errors by omitting the easily confused characters \n'O', '0', 'I', 'l'\n.\n\n\nencode(digest)\n#\n\n\nThe \nencode\n function accepts a 9-byte \nISCC\n Component Digest\n and returns the Base58-\nISCC\n encoded  alphanumeric string of 13 characters which we call the \nISCC\n-Component Code\n.\n\n\ndecode(code)\n#\n\n\nthe \ndecode\n function accepts a 13-\ncharacter\n \nISCC\n-Component Code\n and returns the corresponding 9-byte \nISCC\n-Component Digest\n.\n\n\nReference Code (BI):\n#\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\nSYMBOLS\n \n=\n \n\"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\"\n\n\nVALUES\n \n=\n \n''\n.\njoin\n([\nchr\n(\ni\n)\n \nfor\n \ni\n \nin\n \nrange\n(\n58\n)])\n\n\nV2CTABLE\n \n=\n \nstr\n.\nmaketrans\n(\nVALUES\n,\n \nSYMBOLS\n)\n\n\nC2VTABLE\n \n=\n \nstr\n.\nmaketrans\n(\nSYMBOLS\n,\n \nVALUES\n)\n\n\n\ndef\n \nencode\n(\ndigest\n:\n \nbytes\n)\n \n->\n \nstr\n:\n\n    \nif\n \nlen\n(\ndigest\n)\n \n==\n \n9\n:\n\n        \nreturn\n \nencode\n(\ndigest\n[:\n1\n])\n \n+\n \nencode\n(\ndigest\n[\n1\n:])\n\n    \nassert\n \nlen\n(\ndigest\n)\n \nin\n \n(\n1\n,\n \n8\n),\n \n\"Digest must be 1, 8 or 9 bytes long\"\n\n    \ndigest\n \n=\n \nreversed\n(\ndigest\n)\n\n    \nvalue\n \n=\n \n0\n\n    \nnumvalues\n \n=\n \n1\n\n    \nfor\n \noctet\n \nin\n \ndigest\n:\n\n        \noctet\n \n*=\n \nnumvalues\n\n        \nvalue\n \n+=\n \noctet\n\n        \nnumvalues\n \n*=\n \n256\n\n    \nchars\n \n=\n \n[]\n\n    \nwhile\n \nnumvalues\n \n>\n \n0\n:\n\n        \nchars\n.\nappend\n(\nvalue\n \n%\n \n58\n)\n\n        \nvalue\n \n//=\n \n58\n\n        \nnumvalues\n \n//=\n \n58\n\n    \nreturn\n \nstr\n.\ntranslate\n(\n''\n.\njoin\n([\nchr\n(\nc\n)\n \nfor\n \nc\n \nin\n \nreversed\n(\nchars\n)]),\n \nV2CTABLE\n)\n\n\n\ndef\n \ndecode\n(\ncode\n:\n \nstr\n)\n \n->\n \nbytes\n:\n\n    \nn\n \n=\n \nlen\n(\ncode\n)\n\n    \nif\n \nn\n \n==\n \n13\n:\n\n        \nreturn\n \ndecode\n(\ncode\n[:\n2\n])\n \n+\n \ndecode\n(\ncode\n[\n2\n:])\n\n    \nif\n \nn\n \n==\n \n2\n:\n\n        \nbit_length\n \n=\n \n8\n\n    \nelif\n \nn\n \n==\n \n11\n:\n\n        \nbit_length\n \n=\n \n64\n\n    \nelse\n:\n\n        \nraise\n \nValueError\n(\n'Code must be 2, 11 or 13 chars. Not \n%s\n'\n \n%\n \nn\n)\n\n    \ncode\n \n=\n \nreversed\n(\nstr\n.\ntranslate\n(\ncode\n,\n \nC2VTABLE\n))\n\n    \nvalue\n \n=\n \n0\n\n    \nnumvalues\n \n=\n \n1\n\n    \nfor\n \nc\n \nin\n \ncode\n:\n\n        \nc\n \n=\n \nord\n(\nc\n)\n\n        \nc\n \n*=\n \nnumvalues\n\n        \nvalue\n \n+=\n \nc\n\n        \nnumvalues\n \n*=\n \n58\n\n    \nnumvalues\n \n=\n \n2\n \n**\n \nbit_length\n\n    \ndata\n \n=\n \n[]\n\n    \nwhile\n \nnumvalues\n \n>\n \n1\n:\n\n        \ndata\n.\nappend\n(\nvalue\n \n%\n \n256\n)\n\n        \nvalue\n \n//=\n \n256\n\n        \nnumvalues\n \n//=\n \n256\n\n    \nreturn\n \nbytes\n(\nreversed\n(\ndata\n))\n\n\n\n\n\n\n\nNormalize Text\n#\n\n\nWe define a text normalization function that is specific to our application. It takes unicode text as an input and returns \nnormalized\n Unicode text for further algorithmic processing. We reference this function by the name \nnormalize_text\n. The \nnormalize_text\n function performs the following operations in the given order while each step works with the results of the previous operation:\n\n\n\n\nDecompose the input text by applying \nUnicode Normalization Form D (NFD)\n.\n\n\nReplace each group of one or more consecutive \nSeparator\n characters (\nUnicode categories\n Zs, Zl and Zp) with exactly one Unicode \nSPACE\n \ncharacter\n (\nU+0020\n) .\n\n\nRemove any leading or trailing \nSeparator\n characters.\n\n\nRemove each \ncharacter\n that is not in one of the Unicode categories \nSeparator\n , \nLetter\n, \nNumber\n or \nSymbol\n.\n\n\nConvert all characters to lower case.\n\n\nRe-Compose the text by applying \nUnicode Normalization Form C (NFC)\n.\n\n\nReturn the resulting text.\n\n\n\n\nSimilarity Hash\n#\n\n\nThe \nsimilarity_hash\n function takes a sequence of hash digests (raw 8-bit bytes) which represent a set of features. Each of the digests must be of equal size. The function returns a new hash digest (raw 8-bit bytes) of the same size. For each bit in the input hashes calulate the number of hashes with that bit set and substract the the count of hashes where it is not set. For the output hash set the same bit position to \n0\n if the count is negative or \n1\n if it is zero or positive. The resulting hash digest will retain similarity for similar sets of input hashes. See also  \n[Charikar2002]\n.\n\n\nDiagram (SH)\n#\n\n\n\n\nReference Code (SH)\n#\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndef\n \nsimilarity_hash\n(\nhash_digests\n:\n \nSequence\n[\nByteString\n])\n \n->\n \nByteString\n:\n\n    \nn_bytes\n \n=\n \nlen\n(\nhash_digests\n[\n0\n])\n\n    \nn_bits\n \n=\n \n(\nn_bytes\n \n*\n \n8\n)\n\n    \nvector\n \n=\n \n[\n0\n]\n \n*\n \nn_bits\n\n    \nfor\n \ndigest\n \nin\n \nhash_digests\n:\n\n        \nassert\n \nlen\n(\ndigest\n)\n \n==\n \nn_bytes\n\n        \nh\n \n=\n \nint\n.\nfrom_bytes\n(\ndigest\n,\n \n'big'\n,\n \nsigned\n=\nFalse\n)\n\n        \nfor\n \ni\n \nin\n \nrange\n(\nn_bits\n):\n\n            \nvector\n[\ni\n]\n \n+=\n \nh\n \n&\n \n1\n\n            \nh\n \n>>=\n \n1\n\n    \nminfeatures\n \n=\n \nlen\n(\nhash_digests\n)\n \n*\n \n1.\n \n/\n \n2\n\n    \nshash\n \n=\n \n0\n\n    \nfor\n \ni\n \nin\n \nrange\n(\nn_bits\n):\n\n        \nshash\n \n|=\n \nint\n(\nvector\n[\ni\n]\n \n>=\n \nminfeatures\n)\n \n<<\n \ni\n\n    \nreturn\n \nshash\n.\nto_bytes\n(\nn_bytes\n,\n \n'big'\n,\n \nsigned\n=\nFalse\n)\n\n\n\n\n\n\n\nMinimum Hash\n#\n\n\nThe \nminimum_hash\n function takes an arbitrary sized set of 32-bit integer features and reduces it to a fixed size vector of 128 features such that it preserves similarity with other sets. It is based on the MinHash implementation of the \ndatasketch\n library by \nEric Zhu\n.\n\n\nReference Code (MH)\n#\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\ndef\n \nminimum_hash\n(\nfeatures\n:\n \nSequence\n[\nint\n])\n \n->\n \nList\n[\nint\n]:\n\n    \nmax_int64\n \n=\n \n(\n1\n \n<<\n \n64\n)\n \n-\n \n1\n\n    \nmersenne_prime\n \n=\n \n(\n1\n \n<<\n \n61\n)\n \n-\n \n1\n\n    \nmax_hash\n \n=\n \n(\n1\n \n<<\n \n32\n)\n \n-\n \n1\n\n    \nhashvalues\n \n=\n \n[\nmax_hash\n]\n \n*\n \n128\n\n    \npermutations\n \n=\n \ndeepcopy\n(\nMINHASH_PERMUTATIONS\n)\n\n    \na\n,\n \nb\n \n=\n \npermutations\n\n    \nfor\n \nhv\n \nin\n \nfeatures\n:\n\n        \nnhs\n \n=\n \n[]\n\n        \nfor\n \nx\n \nin\n \nrange\n(\n128\n):\n\n            \nnh\n \n=\n \n(((\na\n[\nx\n]\n \n*\n \nhv\n \n+\n \nb\n[\nx\n])\n \n&\n \nmax_int64\n)\n \n%\n \nmersenne_prime\n)\n \n&\n \nmax_hash\n\n            \nnhs\n.\nappend\n(\nmin\n(\nnh\n,\n \nhashvalues\n[\nx\n]))\n\n        \nhashvalues\n \n=\n \nnhs\n\n    \nreturn\n \nhashvalues\n\n\n\n\n\n\n\nConformance Testing\n#\n\n\nAn application that claims \nISCC\n conformance must pass the \nISCC\n conformance test suite. The test suite is available as json data in our \nGithub Repository\n. Testdata is stuctured as follows:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n{\n\n    \n\"<function_name>\"\n:\n \n{\n\n        \n\"<test_name>\"\n:\n \n{\n\n            \n\"inputs\"\n:\n \n[\n\"<func_input_value1>\"\n,\n \n\"<func_input_value2>\"\n],\n\n            \n\"outputs\"\n:\n \n[\n\"<func_output_value1>\"\n,\n \n\"<func_output_value2>\"\n]\n\n        \n}\n\n    \n}\n\n\n}",
            "title": "Specification"
        },
        {
            "location": "/specification/#iscc-specification-v091",
            "text": "Attention  This document is a work in progress draft! It may be updated, replaced, or obsoleted by other documents at any time. This document must not be used as reference material or cited other than as \"work in progress\".",
            "title": "ISCC - Specification v0.9.1"
        },
        {
            "location": "/specification/#abstract",
            "text": "The  International Standard Content Code ( ISCC ) , is an open and decentralized digital media identifier. An  ISCC  can be created from digital content and its basic metadata by anybody who follows the procedures of the  ISCC  specification or by using open source software that supports  ISCC  creation  conforming to the  ISCC  specification .",
            "title": "Abstract"
        },
        {
            "location": "/specification/#note-to-readers",
            "text": "For public discussion of issues for this draft please use the Github issue tracker:  https://github.com/coblo/iscc-specs/issues .  The latest published version of this draft can be found at  http://iscc.codes/specification/ .   Public review, discussion and contributions are welcome.",
            "title": "Note to Readers"
        },
        {
            "location": "/specification/#about-this-document",
            "text": "This document proposes an open and vendor neutral  ISCC  standard and describes the technical procedures to create and manage  ISCC  identifiers. The first version of this document is produced by the  Content Blockchain Project  as a prototype and received funding from the  Google Digital News Initiative (DNI) . The content of this document is determined by its authors in an open and public consensus process.",
            "title": "About this Document"
        },
        {
            "location": "/specification/#conventions-and-terminology",
            "text": "The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in  RFC 2119  [RFC2119].",
            "title": "Conventions and Terminology"
        },
        {
            "location": "/specification/#definitions",
            "text": "Basic Metadata:  Minimal set of required metadata about the digital media object that is identified by an  ISCC .  Character:  Throughout this specification a  character  is meant to be interpreted as one Unicode code point. This also means that due to the structure of Unicode a  character  is not necessarily a full glyph but might be a combining accent or similar.  Digital Media Object:  A blob of raw bytes with some media type specific encoding.  Extended Metadata:  Metadata that is not encoded within the  ISCC  but may be supplied together with the  ISCC .  Generic Media Type:  A basic content type such as plain text in a normalized and  generic  ( UTF-8 ) encoding format.  ISCC :  International Standard Content Code  ISCC Code :  The printable text encoded representation of an  ISCC  ISCC Digest :  The raw binary data of an  ISCC",
            "title": "Definitions"
        },
        {
            "location": "/specification/#introduction",
            "text": "An  ISCC  permanently identifies the content of a given digital media object at multiple levels of  granularity . It is algorithmically generated from basic metadata and the contents of the digital media object which it identifies. It is designed for being registered and stored on a public and decentralized blockchain. An  ISCC  for a media object can be created and registered by the content author, a publisher, a service provider or anybody else. By itself the  ISCC  and its basic registration on a blockchain does not make any statement or claim about authorship or ownership of the identified content.",
            "title": "Introduction"
        },
        {
            "location": "/specification/#iscc-structure",
            "text": "The  ISCC Digest  is a fixed size sequence of 36 bytes (288 bits) assembled from multiple sub-components. The printable  ISCC Code  is a 52  character  encoded string representation of an  ISCC Digest . This is a high-level overview of the  ISCC  creation process:",
            "title": "ISCC Structure"
        },
        {
            "location": "/specification/#iscc-components",
            "text": "The  ISCC Digest  is built from multiple self-describing 72-bit components:     Components:  Meta-ID  Content-ID  Data-ID  Instance-ID      Context:  Intangible creation  Content similarity  Data similarity  Data checksum    Input:  Metadata  Extracted  content  Raw data  Raw data    Algorithms:  Similarity Hash  Type specific  CDC , Minimum Hash  CDC , Hash Tree    Size:  72 bits  72 bits  72 bits  72 bits     These components may be used independently by applications for various purposes but must be combined into a 52  character   base58-iscc  encoded string (55 with hyphens) for a fully qualified  ISCC Code . The components must be combined in the fixed order of Meta-ID, Content-ID, Data-ID, Instance-ID and may be separated by hyphens.   Examples  Printable output:  ISCC : 11cS7Y9NjD6DX-1DVcUdv5ewjDQ-1Qhwz8x54CShu-1d8uCbWCNbGWg  Machine readable URI:  iscc:11TcMGvUSzqoM1CqVA3ykFawyh1R1sH4Bz8A1of1d2Ju4VjWt26S",
            "title": "ISCC Components"
        },
        {
            "location": "/specification/#component-types",
            "text": "Each component has the same basic structure of a  1-byte header  and a  8-byte body  section.   The 1-byte header of each component is subdivided into 2 nibbles (4 bits). The first nibble specifies the component type while the second nibble is component specific.  The header only needs to be carried in the encoded representation. As similarity searches accross different components are of little use, the type information contained in the header of each component can be safely ignored after an  ISCC  has been decomposed and internaly typed by an application.   The body section of each component is always 8-bytes and can thus be fit into a 64-bit integer for efficient data processing.      Component  Nibble-1  Nibble-2  Byte      Meta-ID  0000  0000 -  ISCC  version (0)  0x00    Content-ID -Text  0001  0000 - Content Type Text  0x10    Content-ID-Text  PCF  0001  0001 - Content Type Text  +  PCF  0x11    Content-ID-Image  0001  0010 - Content Type Image  0x12    Content-ID-Image  PCF  0001  0011 - Content Type Image +  PCF  0x13    Content-ID-Audio  0001  0100 - Content Type Audio  0x14    Content-ID-Audio  PCF  0001  0101 - Content Type Audio +  PCF  0x15    Data-ID  0010  0000 - Reserved  0x20    Instance-ID  0011  0000 - Reserved  0x30",
            "title": "Component Types"
        },
        {
            "location": "/specification/#meta-id-component",
            "text": "The Meta-ID component starts with a 1-byte header  00000000 . The first nibble  0000  indicates that this is a Meta-ID component type. The second nibble  0000  indicates that it belongs to an  ISCC  of version 1. All subsequent components are expected to follow the specification of a version 1  ISCC .  The Meta-ID body is built from a 64-bit  similarity_hash  over 4- character  n-grams of the basic metadata of the content to be identified.  The basic metadata supplied to the META-ID generating function is assumed to be UTF-8 encoded. Errors that occur during the decoding of such a bytestring input to a native Unicode must terminate the process and must not be silenced. An  ISCC  generating application must provide a  meta_id  function that accepts minimal and generic metadata and returns a  Base58- ISCC  encoded  Meta-ID component and trimmed metadata.     Name  Type  Required  Description      title  text  Yes  The title of an intangible creation.    extra  text  No  A short statement that distinguishes this intangible creation from another one. (default: empty string)    version  integer  No  ISCC  version number. (default: 0)      Note  The basic metadata inputs are intentionally simple and generic. We abstain from more specific metadata for Meta-ID generation in favor of compatibility accross industries. Imagine a  creators  input-field for metadata. Who would you list as the creators of a movie? The directors, writers the main actors? Would you list some of them or if not how do you decide whom you will list. All disambiguation of similar title data can be acomplished with the extra-field. Industry- and application-specific metadata requirements can be supplied as extended metadata with  ISCC  registration.",
            "title": "Meta-ID Component"
        },
        {
            "location": "/specification/#generate-meta-id",
            "text": "An  ISCC  generating application must follow these steps in the given order to produce a stable Meta-ID:   Apply Unicode standard  Normalization Form KC (NFKC)  separately to the   title  and  extra  inputs.  Trim  title  and  extra , such that their UTF-8 encoded byte representation does not exceed 128-bytes each.  The results of this step must be supplied as basic metadata for  ISCC  registration.  Concatenate trimmed title  and  extra  from using a space (  \\u0020 ) as a seperator.  Apply  normalize_text  to the results of step 3.  Create a list of 4  character   n-grams  by sliding  character -wise through the result of step 4.  Encode each n-gram from step 5 to an UTF-8 bytestring and calculate its  xxHash64  digest.  Apply  similarity_hash  to the list of digests from step 6.  Prepend the 1-byte component header according to component type and  ISCC  version (e.g.  0x00 ) to the results of step 7.  Encode the resulting 9 byte sequence with  Base58- ISCC  Encoding  Return encoded Meta-ID, trimmed  title  and trimmed  extra  data.    Text trimming  When trimming text be sure to trim the byte-length of the UTF-8 encoded version and not the number of characters. The trim point must be such, that it does not cut into multibyte characters. Characters might have different UTF-8 byte-length. For example  \u00fc  is 2-bytes,  \u9a69  is 3-bytes and  \ud841\udf0e  is 4-bytes. So the trimmed version of a string with 128  \u9a69 -characters will result in a 42- character  string with a 126-byte UTF-8 encoded length. This is necessary because the results of this operation will be stored as basic metadata with strict byte size limits on the blockchain.     Pre-normalization  Applications that perform automated data-ingestion should apply a custimized preliminary normalization to title data tailored to the dataset. Depending on catalog data removing pairs of brackets [], (), {}, and text inbetween them or cutting all text after the first occurence of a semicolon (;) or colon (:) can vastly improve de-duplication.",
            "title": "Generate Meta-ID"
        },
        {
            "location": "/specification/#dealing-with-meta-id-collisions",
            "text": "Ideally we want multiple ISCCs that identify different manifestations of the  same intangible creation  to be automatically grouped by an identical leading Meta-ID component. We call such a natural grouping an  intended component collision . Metadata, captured and edited by humans, is notoriously unreliable. By using normalization and a similarity hash on the metadata we account for some of this variation while keeping the Meta-ID component somewhat stable.   Auto-generated Meta-IDs components are  expected  to miss some intended collisions. An application should check for such  missed intended component collisions  before registering a new Meta-ID with the  canonical registry  of ISCCs by conducting a similarity search and asking for user feedback.  But what about  unintended component collisions ? Such collisions might happen because two  different intangible creations  have very similar or even identical metadata. But they might also happen simply by chance. With 2^56 possibile Meta-ID components the probability of random collisions rises in an S-cuved shape with the number of deployed ISCCs (see:  Hash Collision Probabilities ).  We should keep in mind that, the Meta-ID component is only one part of a fully qualified  ISCC Code . Unintended collisions of the Meta-ID component are generally deemed as  acceptable and expected .   If for any reason an application wants to avoid unintended collisions with pre-existing Meta-ID components it may utilze the  extra -field. An application must first generate a Meta-ID without asking the user for input to the  extra -field and then first check for collisions with the  canonical registry  of ISCCs. After it finds a collision with a pre-existing Meta-ID it may display the metadata of the colliding entry and interact with the user to determine if it indeed is an unintended collision. Only if the user indicates an unintended collision, may the application ask for a disambiguation that is than added as an ammendment to the metadata via the  extra -field to create a different Meta-ID component. The application may repeat the pre-existence check until it finds no collision or a user intended collision. The application must not supply autogenerated input to the  extra -field.  It is our opinion that the concept of  intended collisions  of Meta-ID components is generally usefull concept and a net positive. But one must be aware that this characteristic also has its pitfalls. It is by no means an attempt to provide an unambigous - agreed upon - definition of  \"identical intangible creations\" .",
            "title": "Dealing with Meta-ID collisions"
        },
        {
            "location": "/specification/#content-id",
            "text": "The Content-ID component has multiple subtypes. The subtypes correspond with the  Generic Media Types ( GMT ) . A fully qualified  ISCC  can only have a Content-ID component of one specific  GMT , but there may be multiple ISCCs with different Content-ID types per digital media object.  A Content-ID is generated in two broad steps. In the first step, we extract and convert content from a rich media type to a normalized  GMT . In the second step, we use a  GMT -specific process to generate the Content-ID component of an  ISCC .",
            "title": "Content-ID"
        },
        {
            "location": "/specification/#generic-media-types",
            "text": "The  Content-ID type is signaled by the first 3 bits of the second nibble of the first byte of the Content-ID:     Conent-ID Type  Nibble-2 Bits 0-3      text  000    image  001    audio  010    video  011    mixed  100    Reserved  101, 110, 111",
            "title": "Generic Media Types"
        },
        {
            "location": "/specification/#content-id-text",
            "text": "The Content-ID-Text is built from the extracted plain-text content of an encoded media object. To build a stable Content-ID-Text the plain-text content must first be extracted from the digital media object. It should be extracted in a way that is reproducible. There are many different text document formats out in the wilde and extracting plain-text from all of them is anything but a trivial task. While text-extraction is out of scope for this specification it is recommend, that plain-text content should be extracted with the open-source  Apache Tika v1.17  toolkit, if a generic reproducibility of the Content-ID-Text component is desired.   An  ISCC  generating application must provide a  content_id(text, partial=False)  function that accepts UTF-8 encoded plain text and a boolean indicating the  partial content flag  as input and returns a Content-ID with  GMT  type  text . The procedure to create a Content-ID-Text is as follows:   Apply Unicode standard  Normalization Form KC (NFKC)  to the text input.  Apply  normalize_text  to the text input.  Split the normalized text into a list of words at whitespace boundaries.  Create a list of 5 word shingles by sliding word-wise through the list of words.  Create  a list of 32-bit unsigned integer features by applying  xxHash32  to shingles from step 4.  Apply  minimum_hash  to the list of features from step 5.  Collect the least significant bits from the 128 MinHash features from step 6.  Create two 64-bit digests from the first and second half of the collected bits.  Apply  similarity_hash  to the digests returned from step 8.  Prepend the 1-byte component header ( 0x10  full content or  0x11  partial content).  Encode and return the resulting 9-byte sequence with  Base58- ISCC  Encoding .",
            "title": "Content-ID-Text"
        },
        {
            "location": "/specification/#content-id-image",
            "text": "For the Content-ID-Image we are opting for a DCT-based perceptual image hash instead of a more sophisticated keypoint detection based method. In view of the generic deployabiility of the  ISCC  we chose an algorithm that has moderate computation requirements and is easy to implement while still being robust against most common minor image manipulations.   An  ISCC  generating application must provide a  content_id_image(image, partial=False)  function that accepts a local file path to an image and returns a Content-ID with  GMT  type  image . The procedure to create a Content-ID-Image is as follows:   Convert image to greyscale  Resize the image to 32x32 pixels using  bicubic interpolation  Create a 32x32 two-dimensional array of 8-bit greyscale values from the image data  Perform a discrete cosine transform per row  Perform a DCT per column on the resulting matrix from step 4.  Extract upper left 8x8 corner of array from step 4 as a flat list  Calculate the median of the results from step 5  Create a 64-bit digest by iterating over the values of step 5 and setting a   1 - for values above median and  0  for values below or equal to median.  Prepend the 1-byte component header ( 0x12  full content or  0x13  partial content)  Encode and return the resulting 9-byte sequence with  Base58- ISCC  Encoding    Image Data Input  The  content_id_image  function may optionally accept the raw byte data of an encoded image or an internal native image object as input for convenience.",
            "title": "Content-ID-Image"
        },
        {
            "location": "/specification/#partial-content-flag-pcf",
            "text": "The last bit of the header byte is the \"Partial Content Flag\". It designates if the Content-ID applies to the full content or just some part of it. The  PCF  must be set as a  0 -bit ( full  GMT -specific content ) by default. Setting the  PCF  to  1  enables applications to create multiple ISCCs for partial extracts of one and the same digital file. The exact semantics of  partial content  are outside of the scope of this specification. Applications that plan to support partial Content-IDs should clearly define their semantics. For example, an application might create separate  ISCC  for the text contents of multiple articles of a magazine issue. In such a scenario\nthe Meta-, Data-, and Instance-IDs are the compound key for the magazine issue, while the Content-ID-Text component distinguishes the different articles of the issue. The different Content-ID-Text components would automatically be \"bound\" together by the other 3 components.",
            "title": "Partial Content Flag (PCF)"
        },
        {
            "location": "/specification/#data-id",
            "text": "For the Data-ID that should encode data similarty we use content defined chunking algorithm that provides some shift resistance and calculate the MinHash from those chunks. To accomodate for small files the first 100 chunks have a ~140-byte size target while the remaining chunks target ~ 6kb in size.  The Data-ID is built from the raw encoded data of the content to be identified. An  ISCC  generating application must provide a  data_id  function that accepts the raw encoded data as input. Generate a Data-ID by this procedure:   Apply  chunk_data  to the raw encoded content data.  For each chunk calculate the xxHash32 integer hash.  Apply  minimum_hash  to the resulting list of 32-bit unsigned integers.  Collect the least significant bits from the 128 MinHash features.  Create two 64-bit digests from the first and second half of the collected bits.  Apply  similarity_hash  to the results of step 5.  Prepend the 1-byte component header (e.g. 0x20).  Encode and return the resulting 9-byte sequence with  Base58- ISCC  Encoding .",
            "title": "Data-ID"
        },
        {
            "location": "/specification/#instance-id",
            "text": "The Instance-ID is built from the raw data of the media object to be identified and serves as checksum for the media object. The raw data of the media object is split into 64-kB data-chunks. Then we build a hash-tree from those chunks and use the truncated top-hash (merkle root) as component body of the Instance-ID.  To guard against length-extension attacks and second pre-image attacks we use double sha256 for hashing. We also prefix the hash input data with a  0x00 -byte for the leaf nodes hashes and with a  0x01 -byte for the  internal node hashes. While the Instance-ID itself is a non-cryptographic checksum, the full top-hash may be supplied in the extended metadata of an  ISCC  secure integrity verification is required.   An  ISCC  generating application must provide a  instance_id  function that accepts the raw data file as input and returns an encoded Instance-ID and a full hex-encoded 256-bit top-hash. Generate an Instance-ID by this procedure:   Split the raw bytes of the encoded media object into 64-kB chunks.  For each chunk calculate the  sha256d  of the concatenation of a  0x00 -byte and the chunk bytes. We call the resulting values  leaf node hashes  ( LNH ).  Calculate the next level of the hash tree by applying  sha256d  to the concatenation of a  0x01 -byte and adjacent pairs of  LNH  values. If the length of the list of  LNH  values is uneven concatenate the last  LNH  value with itself. We call the resulting values  internal node hashes  ( INH ).  Recursively apply  0x01 -prefixed pairwise hashing to the results of  step 3 until the process yields only one hash value. We call this value the top-hash.  Trim the resulting  top hash  to the first 8 bytes.  Prepend the 1-byte component header (e.g.  0x30 ).  Encode resulting 9-byte sequence with  Base58- ISCC  Encoding  to an Instance-ID Code  Hex-Encode the  top hash    Return the Intance-ID and the hex-encoded top-hash   Applications may carry, store, and process the leaf node hashes or even the full hash-tree for advanced streaming data identification or partial data integrity verification.",
            "title": "Instance-ID"
        },
        {
            "location": "/specification/#procedures-algorithms",
            "text": "",
            "title": "Procedures &amp; Algorithms"
        },
        {
            "location": "/specification/#base58-iscc-encoding",
            "text": "The  ISCC  uses a custom per-component data encoding that is based on the  zbase62  encoding by  Zooko Wilcox-O'Hearn . The encoding does not require padding and will always yield component codes of 13 characters length for our 72-bit digests. The predictable size of the encoding is a property that allows for easy composition and decomposition of components without having to rely on a delimiter (hyphen) in the  ISCC  code representation. Colliding body segments of the digest are preserved by encoding the header and body separately. The symbol table also minimizes transcription and OCR errors by omitting the easily confused characters  'O', '0', 'I', 'l' .",
            "title": "Base58-ISCC Encoding"
        },
        {
            "location": "/specification/#encodedigest",
            "text": "The  encode  function accepts a 9-byte  ISCC  Component Digest  and returns the Base58- ISCC  encoded  alphanumeric string of 13 characters which we call the  ISCC -Component Code .",
            "title": "encode(digest)"
        },
        {
            "location": "/specification/#decodecode",
            "text": "the  decode  function accepts a 13- character   ISCC -Component Code  and returns the corresponding 9-byte  ISCC -Component Digest .",
            "title": "decode(code)"
        },
        {
            "location": "/specification/#reference-code-bi",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48 SYMBOLS   =   \"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\"  VALUES   =   '' . join ([ chr ( i )   for   i   in   range ( 58 )])  V2CTABLE   =   str . maketrans ( VALUES ,   SYMBOLS )  C2VTABLE   =   str . maketrans ( SYMBOLS ,   VALUES )  def   encode ( digest :   bytes )   ->   str : \n     if   len ( digest )   ==   9 : \n         return   encode ( digest [: 1 ])   +   encode ( digest [ 1 :]) \n     assert   len ( digest )   in   ( 1 ,   8 ),   \"Digest must be 1, 8 or 9 bytes long\" \n     digest   =   reversed ( digest ) \n     value   =   0 \n     numvalues   =   1 \n     for   octet   in   digest : \n         octet   *=   numvalues \n         value   +=   octet \n         numvalues   *=   256 \n     chars   =   [] \n     while   numvalues   >   0 : \n         chars . append ( value   %   58 ) \n         value   //=   58 \n         numvalues   //=   58 \n     return   str . translate ( '' . join ([ chr ( c )   for   c   in   reversed ( chars )]),   V2CTABLE )  def   decode ( code :   str )   ->   bytes : \n     n   =   len ( code ) \n     if   n   ==   13 : \n         return   decode ( code [: 2 ])   +   decode ( code [ 2 :]) \n     if   n   ==   2 : \n         bit_length   =   8 \n     elif   n   ==   11 : \n         bit_length   =   64 \n     else : \n         raise   ValueError ( 'Code must be 2, 11 or 13 chars. Not  %s '   %   n ) \n     code   =   reversed ( str . translate ( code ,   C2VTABLE )) \n     value   =   0 \n     numvalues   =   1 \n     for   c   in   code : \n         c   =   ord ( c ) \n         c   *=   numvalues \n         value   +=   c \n         numvalues   *=   58 \n     numvalues   =   2   **   bit_length \n     data   =   [] \n     while   numvalues   >   1 : \n         data . append ( value   %   256 ) \n         value   //=   256 \n         numvalues   //=   256 \n     return   bytes ( reversed ( data ))",
            "title": "Reference Code (BI):"
        },
        {
            "location": "/specification/#normalize-text",
            "text": "We define a text normalization function that is specific to our application. It takes unicode text as an input and returns  normalized  Unicode text for further algorithmic processing. We reference this function by the name  normalize_text . The  normalize_text  function performs the following operations in the given order while each step works with the results of the previous operation:   Decompose the input text by applying  Unicode Normalization Form D (NFD) .  Replace each group of one or more consecutive  Separator  characters ( Unicode categories  Zs, Zl and Zp) with exactly one Unicode  SPACE   character  ( U+0020 ) .  Remove any leading or trailing  Separator  characters.  Remove each  character  that is not in one of the Unicode categories  Separator  ,  Letter ,  Number  or  Symbol .  Convert all characters to lower case.  Re-Compose the text by applying  Unicode Normalization Form C (NFC) .  Return the resulting text.",
            "title": "Normalize Text"
        },
        {
            "location": "/specification/#similarity-hash",
            "text": "The  similarity_hash  function takes a sequence of hash digests (raw 8-bit bytes) which represent a set of features. Each of the digests must be of equal size. The function returns a new hash digest (raw 8-bit bytes) of the same size. For each bit in the input hashes calulate the number of hashes with that bit set and substract the the count of hashes where it is not set. For the output hash set the same bit position to  0  if the count is negative or  1  if it is zero or positive. The resulting hash digest will retain similarity for similar sets of input hashes. See also   [Charikar2002] .",
            "title": "Similarity Hash"
        },
        {
            "location": "/specification/#diagram-sh",
            "text": "",
            "title": "Diagram (SH)"
        },
        {
            "location": "/specification/#reference-code-sh",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 def   similarity_hash ( hash_digests :   Sequence [ ByteString ])   ->   ByteString : \n     n_bytes   =   len ( hash_digests [ 0 ]) \n     n_bits   =   ( n_bytes   *   8 ) \n     vector   =   [ 0 ]   *   n_bits \n     for   digest   in   hash_digests : \n         assert   len ( digest )   ==   n_bytes \n         h   =   int . from_bytes ( digest ,   'big' ,   signed = False ) \n         for   i   in   range ( n_bits ): \n             vector [ i ]   +=   h   &   1 \n             h   >>=   1 \n     minfeatures   =   len ( hash_digests )   *   1.   /   2 \n     shash   =   0 \n     for   i   in   range ( n_bits ): \n         shash   |=   int ( vector [ i ]   >=   minfeatures )   <<   i \n     return   shash . to_bytes ( n_bytes ,   'big' ,   signed = False )",
            "title": "Reference Code (SH)"
        },
        {
            "location": "/specification/#minimum-hash",
            "text": "The  minimum_hash  function takes an arbitrary sized set of 32-bit integer features and reduces it to a fixed size vector of 128 features such that it preserves similarity with other sets. It is based on the MinHash implementation of the  datasketch  library by  Eric Zhu .",
            "title": "Minimum Hash"
        },
        {
            "location": "/specification/#reference-code-mh",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 def   minimum_hash ( features :   Sequence [ int ])   ->   List [ int ]: \n     max_int64   =   ( 1   <<   64 )   -   1 \n     mersenne_prime   =   ( 1   <<   61 )   -   1 \n     max_hash   =   ( 1   <<   32 )   -   1 \n     hashvalues   =   [ max_hash ]   *   128 \n     permutations   =   deepcopy ( MINHASH_PERMUTATIONS ) \n     a ,   b   =   permutations \n     for   hv   in   features : \n         nhs   =   [] \n         for   x   in   range ( 128 ): \n             nh   =   ((( a [ x ]   *   hv   +   b [ x ])   &   max_int64 )   %   mersenne_prime )   &   max_hash \n             nhs . append ( min ( nh ,   hashvalues [ x ])) \n         hashvalues   =   nhs \n     return   hashvalues",
            "title": "Reference Code (MH)"
        },
        {
            "location": "/specification/#conformance-testing",
            "text": "An application that claims  ISCC  conformance must pass the  ISCC  conformance test suite. The test suite is available as json data in our  Github Repository . Testdata is stuctured as follows:  1\n2\n3\n4\n5\n6\n7\n8 { \n     \"<function_name>\" :   { \n         \"<test_name>\" :   { \n             \"inputs\" :   [ \"<func_input_value1>\" ,   \"<func_input_value2>\" ], \n             \"outputs\" :   [ \"<func_output_value1>\" ,   \"<func_output_value2>\" ] \n         } \n     }  }",
            "title": "Conformance Testing"
        },
        {
            "location": "/contributing/",
            "text": "",
            "title": "Contributing"
        },
        {
            "location": "/license/",
            "text": "License\n#\n\n\nCC BY-NC-SA 4.0 License\n\n\nCopyright \u00a9 2016 - 2017 Content Blockchain Project\n\n\nThis work is licensed under a \nCreative Commons Attribution-ShareAlike 4.0 International License\n.",
            "title": "License"
        },
        {
            "location": "/license/#license",
            "text": "CC BY-NC-SA 4.0 License  Copyright \u00a9 2016 - 2017 Content Blockchain Project  This work is licensed under a  Creative Commons Attribution-ShareAlike 4.0 International License .",
            "title": "License"
        }
    ]
}